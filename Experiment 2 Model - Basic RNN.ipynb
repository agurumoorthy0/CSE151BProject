{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "718c38cf",
   "metadata": {},
   "source": [
    "## Install the package dependencies before running this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ac7530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    number of trajectories in each city\\n    # austin --  train: 43041 test: 6325 \\n    # miami -- train: 55029 test:7971\\n    # pittsburgh -- train: 43544 test: 6361\\n    # dearborn -- train: 24465 test: 3671\\n    # washington-dc -- train: 25744 test: 3829\\n    # palo-alto -- train:  11993 test:1686\\n\\n    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\\n    \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "    number of trajectories in each city\n",
    "    # austin --  train: 43041 test: 6325 \n",
    "    # miami -- train: 55029 test:7971\n",
    "    # pittsburgh -- train: 43544 test: 6361\n",
    "    # dearborn -- train: 24465 test: 3671\n",
    "    # washington-dc -- train: 25744 test: 3829\n",
    "    # palo-alto -- train:  11993 test:1686\n",
    "\n",
    "    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b472cf2",
   "metadata": {},
   "source": [
    "## Create training, validation, and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "091abbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code is inspired by the code from the Week 7 Discussion\n",
    "from glob import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "ROOT_PATH = \"./\"\n",
    "\n",
    "cities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"]\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "def get_city_trajectories(city=\"palo-alto\", split=\"train\", normalized=False):\n",
    "    f_in = ROOT_PATH + \"train\" + \"/\" + city + \"_inputs\"\n",
    "    f_out = ROOT_PATH + \"train\" + \"/\" + city + \"_outputs\"\n",
    "    \n",
    "    inputs = None\n",
    "    outputs = None\n",
    "    \n",
    "    if city==\"all\":\n",
    "        allInputs = np.zeros((0,50,2))\n",
    "        allOutputs = np.zeros((0,60,2))\n",
    "        for city in cities:\n",
    "            if split==\"train\":\n",
    "                f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "                inputs = pickle.load(open(f_in, \"rb\"))\n",
    "                n = len(inputs)\n",
    "                allInputs = np.concatenate((allInputs, np.asarray(inputs)[:int(n * 0.8)]))\n",
    "\n",
    "                f_out = ROOT_PATH + split + \"/\" + city + \"_outputs\"\n",
    "                outputs = pickle.load(open(f_out, \"rb\"))\n",
    "                allOutputs = np.concatenate((allOutputs, np.asarray(outputs)[:int(n * 0.8)]))\n",
    "\n",
    "            elif split == 'val':\n",
    "                f_in = ROOT_PATH + 'train' + \"/\" + city + \"_inputs\"\n",
    "                inputs = pickle.load(open(f_in, \"rb\"))\n",
    "                n = len(inputs)\n",
    "                allInputs = np.concatenate((allInputs, np.asarray(inputs)[int(n * 0.8):]))\n",
    "\n",
    "                f_out = ROOT_PATH + 'train' + \"/\" + city + \"_outputs\"\n",
    "                outputs = pickle.load(open(f_out, \"rb\"))\n",
    "                allOutputs = np.concatenate((allOutputs, np.asarray(outputs)[int(n * 0.8):]))\n",
    "\n",
    "            else:\n",
    "                f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "                inputs = pickle.load(open(f_in, \"rb\"))\n",
    "                n = len(inputs)\n",
    "                allInputs = np.concatenate((allInputs, np.asarray(inputs)))\n",
    "                \n",
    "        if (normalized):\n",
    "            allInputs = (allInputs - np.min(allInputs))/(np.max(allInputs) - np.min(allInputs))\n",
    "            allOutputs = (allOutputs - np.min(allOutputs))/(np.max(allOutputs) - np.min(allOutputs))\n",
    "        return allInputs, allOutputs\n",
    "    \n",
    "    if split==\"train\":\n",
    "        inputs = pickle.load(open(f_in, \"rb\"))\n",
    "        n = len(inputs)\n",
    "        inputs = np.asarray(inputs)[:int(n * 0.8)]\n",
    "        \n",
    "        outputs = pickle.load(open(f_out, \"rb\"))\n",
    "        outputs = np.asarray(outputs)[:int(n * 0.8)]\n",
    "    \n",
    "    elif split==\"val\":\n",
    "        inputs = pickle.load(open(f_in, \"rb\"))\n",
    "        n = len(inputs)\n",
    "        inputs = np.asarray(inputs)[int(n * 0.8):]\n",
    "        \n",
    "        outputs = pickle.load(open(f_out, \"rb\"))\n",
    "        outputs = np.asarray(outputs)[int(n * 0.8):]\n",
    "    \n",
    "    else:\n",
    "        f_in = ROOT_PATH + spiit + \"/\" + city + \"_inputs\"\n",
    "        f_out = ROOT_PATH + split + \"/\" + city + \"_outputs\"\n",
    "        inputs = pickle.load(open(f_in, \"rb\"))\n",
    "        n = len(inputs)\n",
    "        inputs = np.asarray(inputs)\n",
    "        return inputs\n",
    "\n",
    "    return inputs, outputs\n",
    "\n",
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, city: str, split:str, transform=None):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.transform = transform\n",
    "\n",
    "        self.inputs, self.outputs = get_city_trajectories(city=city, split=split, normalized=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        data = (self.inputs[idx], self.outputs[idx])\n",
    "            \n",
    "        if self.transform:\n",
    "            data = torch.nn.functional.normalize(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "# intialize a dataset\n",
    "city = 'all' \n",
    "train_dataset  = ArgoverseDataset(city = city, split = \"train\")\n",
    "val_dataset = ArgoverseDataset(city = city, split = \"val\")\n",
    "test_dataset= get_city_trajectories(city = city, split = \"test\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058453cc",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c14f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1  # batch size \n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04f672e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is inspired by the code from the Week 4 Discussion\n",
    "from torch import nn, optim\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.inputToHidden = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.inputToOutput = nn.Linear(input_size + hidden_size, output_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, x, hidden_state):\n",
    "        combined = torch.cat((x, hidden_state), 1)\n",
    "        hidden = torch.sigmoid(self.inputToHidden(combined))\n",
    "        output = self.inputToOutput(combined)\n",
    "        return output, hidden\n",
    "        \n",
    "    def init_state(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac3de7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.00001\n",
    "num_epochs = 1\n",
    "hidden_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b77ca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "model = Model(2, hidden_size, 2)\n",
    "opt = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819d015a",
   "metadata": {},
   "source": [
    "## Train and Validate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cd4c8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0  RMSE: 2.4499981194057643 MSE: 6.002490785091781\n",
      "step 1000  RMSE: 113.07017086644231 MSE: 12784.863539766458\n",
      "step 2000  RMSE: 121.2331578872693 MSE: 14697.478571319565\n",
      "step 3000  RMSE: 123.52305762847568 MSE: 15257.945765887724\n",
      "step 4000  RMSE: 124.21106198610012 MSE: 15428.387919714807\n",
      "step 5000  RMSE: 124.40049093812509 MSE: 15475.482145646543\n",
      "step 6000  RMSE: 124.41709253848543 MSE: 15479.61291573005\n",
      "step 7000  RMSE: 124.41782948808824 MSE: 15479.796294527\n",
      "step 8000  RMSE: 124.41784535095987 MSE: 15479.800241775367\n",
      "step 9000  RMSE: 124.41785460623339 MSE: 15479.80254481783\n",
      "step 10000  RMSE: 124.41786317832056 MSE: 15479.804677859294\n",
      "step 11000  RMSE: 124.41787107215336 MSE: 15479.806642126976\n",
      "step 12000  RMSE: 124.41787906326468 MSE: 15479.808630601156\n",
      "step 13000  RMSE: 124.41788556621087 MSE: 15479.810248766742\n",
      "step 14000  RMSE: 124.41789183983933 MSE: 15479.811809869956\n",
      "step 15000  RMSE: 124.4179012886069 MSE: 15479.81416106153\n",
      "step 16000  RMSE: 124.41793119450413 MSE: 15479.821602720365\n",
      "step 17000  RMSE: 124.41800285769554 MSE: 15479.839435097534\n",
      "step 18000  RMSE: 124.41803435958032 MSE: 15479.847273901709\n",
      "step 19000  RMSE: 124.41812352643755 MSE: 15479.869461839873\n",
      "step 20000  RMSE: 124.41813011317747 MSE: 15479.871100859556\n",
      "step 21000  RMSE: 124.41818552499436 MSE: 15479.884889331914\n",
      "step 22000  RMSE: 124.41824159843476 MSE: 15479.898842446482\n",
      "step 23000  RMSE: 124.41828578502125 MSE: 15479.909837683223\n",
      "step 24000  RMSE: 124.41838439419045 MSE: 15479.934375260536\n",
      "step 25000  RMSE: 124.41842449754824 MSE: 15479.944354452115\n",
      "step 26000  RMSE: 124.41846526346495 MSE: 15479.954498516034\n",
      "step 27000  RMSE: 124.41850803438653 MSE: 15479.965141502707\n",
      "step 28000  RMSE: 124.41860093151347 MSE: 15479.988257755203\n",
      "step 29000  RMSE: 124.41866818123695 MSE: 15480.004991992744\n",
      "step 30000  RMSE: 124.41875755099547 MSE: 15480.027230533393\n",
      "step 31000  RMSE: 124.41877966385127 MSE: 15480.03273304197\n",
      "step 32000  RMSE: 124.41884733850746 MSE: 15480.049573042827\n",
      "step 33000  RMSE: 124.41889309794576 MSE: 15480.060959718056\n",
      "step 34000  RMSE: 124.41907817064087 MSE: 15480.107012832043\n",
      "step 35000  RMSE: 124.41921334568922 MSE: 15480.140649560128\n",
      "step 36000  RMSE: 124.41936180112333 MSE: 15480.177590998828\n",
      "step 37000  RMSE: 124.41947264631858 MSE: 15480.205173588018\n",
      "step 38000  RMSE: 124.41947474909617 MSE: 15480.205696840978\n",
      "step 39000  RMSE: 124.41961072182784 MSE: 15480.239532171176\n",
      "step 40000  RMSE: 124.4196149603705 MSE: 15480.240586886848\n",
      "step 41000  RMSE: 124.41987469998907 MSE: 15480.305220360979\n",
      "step 42000  RMSE: 124.41995824164518 MSE: 15480.326008852728\n",
      "step 43000  RMSE: 124.42001163256332 MSE: 15480.33929464719\n",
      "step 44000  RMSE: 124.42005391882971 MSE: 15480.349817164493\n",
      "step 45000  RMSE: 124.42042734163905 MSE: 15480.442739876082\n",
      "step 46000  RMSE: 124.42053303274241 MSE: 15480.469040151746\n",
      "step 47000  RMSE: 124.42059961519806 MSE: 15480.485608605422\n",
      "step 48000  RMSE: 124.42062494587391 MSE: 15480.491911921821\n",
      "step 49000  RMSE: 124.42070213208858 MSE: 15480.511119041912\n",
      "step 50000  RMSE: 124.4210208650794 MSE: 15480.590433108524\n",
      "step 51000  RMSE: 124.42102177208 MSE: 15480.590658808404\n",
      "step 52000  RMSE: 124.42123803380136 MSE: 15480.644473863858\n",
      "step 53000  RMSE: 124.4212399292918 MSE: 15480.644945542395\n",
      "step 54000  RMSE: 124.42128891167079 MSE: 15480.657134441451\n",
      "step 55000  RMSE: 124.42136268093881 MSE: 15480.675491381715\n",
      "step 56000  RMSE: 124.42175500555129 MSE: 15480.773118661427\n",
      "step 57000  RMSE: 124.4218011549634 MSE: 15480.784602645253\n",
      "step 58000  RMSE: 124.42180472302293 MSE: 15480.78549053405\n",
      "step 59000  RMSE: 124.42186375380713 MSE: 15480.800179970945\n",
      "step 60000  RMSE: 124.42213986012464 MSE: 15480.868887372417\n",
      "step 61000  RMSE: 124.42214055999375 MSE: 15480.86906153084\n",
      "step 62000  RMSE: 124.42243162519848 MSE: 15480.941491527192\n",
      "step 63000  RMSE: 124.42243173249307 MSE: 15480.941518226897\n",
      "step 64000  RMSE: 124.42246241501071 MSE: 15480.949153414755\n",
      "step 65000  RMSE: 124.42251893367434 MSE: 15480.96321780055\n",
      "step 66000  RMSE: 124.42261919171109 MSE: 15480.988166525553\n",
      "step 67000  RMSE: 124.42271603404157 MSE: 15481.012265287747\n",
      "step 68000  RMSE: 124.42272844525883 MSE: 15481.015353762621\n",
      "step 69000  RMSE: 124.42295463906996 MSE: 15481.071641116061\n",
      "step 70000  RMSE: 124.42311358013554 MSE: 15481.111192975308\n",
      "step 71000  RMSE: 124.4231146094035 MSE: 15481.11144910476\n",
      "step 72000  RMSE: 124.4231691057514 MSE: 15481.125010318408\n",
      "step 73000  RMSE: 124.423393360557 MSE: 15481.1808153559\n",
      "step 74000  RMSE: 124.42347492364017 MSE: 15481.201112073713\n",
      "step 75000  RMSE: 124.42351494852616 MSE: 15481.211072146112\n",
      "step 76000  RMSE: 124.42371074534567 MSE: 15481.259795641447\n",
      "step 77000  RMSE: 124.42377936869602 MSE: 15481.276872389946\n",
      "step 78000  RMSE: 124.42385065594817 MSE: 15481.294612053693\n",
      "step 79000  RMSE: 124.42395721233298 MSE: 15481.321128376467\n",
      "step 80000  RMSE: 124.42398777464447 MSE: 15481.328733744878\n",
      "step 81000  RMSE: 124.4240006558572 MSE: 15481.331939208754\n",
      "step 82000  RMSE: 124.42408139669237 MSE: 15481.352031410728\n",
      "step 83000  RMSE: 124.42408141435594 MSE: 15481.352035806274\n",
      "step 84000  RMSE: 124.42418056101128 MSE: 15481.376708279136\n",
      "step 85000  RMSE: 124.42423327214996 MSE: 15481.38982536239\n",
      "step 86000  RMSE: 124.42423589848275 MSE: 15481.390478921285\n",
      "step 87000  RMSE: 124.42440016343974 MSE: 15481.431356031782\n",
      "step 88000  RMSE: 124.42446091373772 MSE: 15481.446473674247\n",
      "step 89000  RMSE: 124.42447839115033 MSE: 15481.450822909836\n",
      "step 90000  RMSE: 124.42464241928305 MSE: 15481.49164116645\n",
      "step 91000  RMSE: 124.42474677611615 MSE: 15481.517610300627\n",
      "step 92000  RMSE: 124.42475354839297 MSE: 15481.519295578328\n",
      "step 93000  RMSE: 124.42479339192066 MSE: 15481.529210622142\n",
      "step 94000  RMSE: 124.4248164087596 MSE: 15481.534938353532\n",
      "step 95000  RMSE: 124.4248962424841 MSE: 15481.554804952933\n",
      "step 96000  RMSE: 124.42492231714559 MSE: 15481.561293627714\n",
      "step 97000  RMSE: 124.42497868824302 MSE: 15481.575321569728\n",
      "step 98000  RMSE: 124.42504071282208 MSE: 15481.590756387433\n",
      "step 99000  RMSE: 124.42519469006939 MSE: 15481.629073661672\n",
      "step 100000  RMSE: 124.4252857917184 MSE: 15481.6517443508\n",
      "step 101000  RMSE: 124.42528999649858 MSE: 15481.65279071277\n",
      "step 102000  RMSE: 124.42555460868026 MSE: 15481.718639677672\n",
      "step 103000  RMSE: 124.42560926985797 MSE: 15481.732242175365\n",
      "step 104000  RMSE: 124.42562340267179 MSE: 15481.735759143505\n",
      "step 105000  RMSE: 124.42571555261276 MSE: 15481.7586907797\n",
      "step 106000  RMSE: 124.42574456959458 MSE: 15481.765911697998\n",
      "step 107000  RMSE: 124.42587159885501 MSE: 15481.797523134755\n",
      "step 108000  RMSE: 124.42601138253387 MSE: 15481.832308566447\n",
      "step 109000  RMSE: 124.42608303636207 MSE: 15481.85013977167\n",
      "step 110000  RMSE: 124.42608312147674 MSE: 15481.85016095264\n",
      "step 111000  RMSE: 124.42617697914477 MSE: 15481.873517645456\n",
      "step 112000  RMSE: 124.4262003668131 MSE: 15481.879337722323\n",
      "step 113000  RMSE: 124.4262356954182 MSE: 15481.888129331763\n",
      "step 114000  RMSE: 124.42645327659108 MSE: 15481.942274991703\n",
      "step 115000  RMSE: 124.4265006673675 MSE: 15481.954068326406\n",
      "step 116000  RMSE: 124.42687190772966 MSE: 15482.046452742567\n",
      "step 117000  RMSE: 124.42687192500675 MSE: 15482.046457042032\n",
      "step 118000  RMSE: 124.42688029108879 MSE: 15482.04853897294\n",
      "step 119000  RMSE: 124.4274768361797 MSE: 15482.196991818033\n",
      "step 120000  RMSE: 124.42749811921338 MSE: 15482.202288206849\n",
      "step 121000  RMSE: 124.42756048433347 MSE: 15482.217808082465\n",
      "step 122000  RMSE: 124.42885899571215 MSE: 15482.540950974817\n",
      "step 123000  RMSE: 124.42885900744915 MSE: 15482.540953895657\n",
      "step 124000  RMSE: 124.4288590298366 MSE: 15482.540959466949\n",
      "step 125000  RMSE: 124.42888565789447 MSE: 15482.547586065375\n",
      "step 126000  RMSE: 124.42919751752645 MSE: 15482.62519485561\n",
      "step 127000  RMSE: 124.42925302014031 MSE: 15482.639007150095\n",
      "step 128000  RMSE: 124.42935936275833 MSE: 15482.665471426453\n",
      "step 129000  RMSE: 124.42944956876603 MSE: 15482.687919986089\n",
      "step 130000  RMSE: 124.42950355450687 MSE: 15482.701354821038\n",
      "step 131000  RMSE: 124.4296063323535 MSE: 15482.726932024467\n",
      "step 132000  RMSE: 124.42976818470537 MSE: 15482.767210499516\n",
      "step 133000  RMSE: 124.4297694892123 MSE: 15482.76753513851\n",
      "step 134000  RMSE: 124.42976954185279 MSE: 15482.767548238598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 135000  RMSE: 124.42978055707813 MSE: 15482.770289482618\n",
      "step 136000  RMSE: 124.42985015211221 MSE: 15482.787608877097\n",
      "step 137000  RMSE: 124.42985087112926 MSE: 15482.787787811467\n",
      "step 138000  RMSE: 124.42990984688961 MSE: 15482.802464505075\n",
      "step 139000  RMSE: 124.42993263767114 MSE: 15482.808136215379\n",
      "step 140000  RMSE: 124.42999677839272 MSE: 15482.824098270823\n",
      "step 141000  RMSE: 124.43001862506371 MSE: 15482.829535033703\n",
      "step 142000  RMSE: 124.4300595130471 MSE: 15482.839710420443\n",
      "step 143000  RMSE: 124.43013996074275 MSE: 15482.859730650029\n",
      "step 144000  RMSE: 124.43017813875151 MSE: 15482.869231641434\n",
      "step 145000  RMSE: 124.43020388750911 MSE: 15482.875639487085\n",
      "step 146000  RMSE: 124.43021379026624 MSE: 15482.87810389136\n",
      "step 147000  RMSE: 124.43028452264623 MSE: 15482.895706386693\n",
      "step 148000  RMSE: 124.43030550956854 MSE: 15482.900929204565\n",
      "step 149000  RMSE: 124.4303912470711 MSE: 15482.92226589919\n",
      "step 150000  RMSE: 124.43041816673602 MSE: 15482.928965148789\n",
      "step 151000  RMSE: 124.43044790072283 MSE: 15482.936364774498\n",
      "step 152000  RMSE: 124.43051939880233 MSE: 15482.954157855722\n",
      "step 153000  RMSE: 124.43051965112922 MSE: 15482.954220650056\n",
      "step 154000  RMSE: 124.43055018816514 MSE: 15482.961820129483\n",
      "step 155000  RMSE: 124.43055025566842 MSE: 15482.961836928424\n",
      "step 156000  RMSE: 124.43057388706481 MSE: 15482.967717864296\n",
      "step 157000  RMSE: 124.43060476576119 MSE: 15482.97540237307\n",
      "step 158000  RMSE: 124.43061897177823 MSE: 15482.978937699856\n",
      "step 159000  RMSE: 124.4306530233981 MSE: 15482.98741182929\n",
      "step 160000  RMSE: 124.4307602752968 MSE: 15483.014102688381\n",
      "step 161000  RMSE: 124.4307691306916 MSE: 15483.016306455474\n",
      "step 162000  RMSE: 124.43077402509081 MSE: 15483.017524483213\n",
      "step 163000  RMSE: 124.43085048747244 MSE: 15483.03655303572\n",
      "1732.0385248661041\n",
      "epoch 0  RMSE: 124.43085048860011 MSE: 15483.036553316357\n"
     ]
    }
   ],
   "source": [
    "#This code is inspired by the code from the Week 4 Discussion\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    if (epoch == 0):\n",
    "        startTime = time.time()\n",
    "    \n",
    "    total_loss = 0\n",
    "    for i_batch, sample_batch in enumerate(train_loader):\n",
    "        hidden_state = model.init_state()\n",
    "        inp, out = sample_batch\n",
    "        inp = inp.reshape(50,2)\n",
    "        out = out.reshape(60,2)\n",
    "        inp = torch.cat((inp, out))\n",
    "        preds = torch.empty(0,2)\n",
    "        for index in range(0, inp.shape[0] - 1):\n",
    "            pred, hidden_state = model(inp[index].reshape(1,2).float(), hidden_state)\n",
    "            preds = torch.cat((preds, pred))\n",
    "        loss = criterion(preds, inp[:109].reshape(109,2).float())        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total_loss += loss.item()\n",
    "        if (i_batch % 1000 == 0):\n",
    "            print('step {}  RMSE: {} MSE: {}'.format(i_batch, math.sqrt(total_loss / len(train_dataset)), total_loss / len(train_dataset)))\n",
    "    if (epoch == 0):\n",
    "        endTime = time.time()\n",
    "        print(endTime - startTime)\n",
    "    print('epoch {}  RMSE: {} MSE: {}'.format(epoch, math.sqrt(total_loss / len(train_dataset)), total_loss / len(train_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34bd8a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Current Loss: 1.0220745257157804 Average Loss: 1.0220745257157804\n",
      "Step: 100 Current Loss: 1.5588431560116875 Average Loss: 11.051068794722688\n",
      "Step: 200 Current Loss: 0.1912475211046974 Average Loss: 15.920719203452219\n",
      "Step: 300 Current Loss: 1.6476144581318188 Average Loss: 19.81061528906282\n",
      "Step: 400 Current Loss: 1.7263631019303247 Average Loss: 22.78998019784148\n",
      "Step: 500 Current Loss: 0.54830514326979 Average Loss: 25.629256597761817\n",
      "Step: 600 Current Loss: 1.1172601406316298 Average Loss: 28.17327526424604\n",
      "Step: 700 Current Loss: 1.8560798804926055 Average Loss: 30.185112445673163\n",
      "Step: 800 Current Loss: 2.0078420244560142 Average Loss: 31.923962620927934\n",
      "Step: 900 Current Loss: 1.3649112359245672 Average Loss: 33.71973154565406\n",
      "Step: 1000 Current Loss: 1.0338913718803853 Average Loss: 35.992182353755155\n",
      "Step: 1100 Current Loss: 2.121155776123972 Average Loss: 37.86860004985395\n",
      "Step: 1200 Current Loss: 0.8831115987533428 Average Loss: 39.11857179364235\n",
      "Step: 1300 Current Loss: 0.8366581680799584 Average Loss: 41.099286261028084\n",
      "Step: 1400 Current Loss: 1.283254095762419 Average Loss: 42.47365750857074\n",
      "Step: 1500 Current Loss: 2.1121910554374317 Average Loss: 44.00542198661012\n",
      "Step: 1600 Current Loss: 0.32336103807062005 Average Loss: 45.392364661720634\n",
      "Step: 1700 Current Loss: 0.8439623299675182 Average Loss: 46.72161998517473\n",
      "Step: 1800 Current Loss: 0.7205449069549151 Average Loss: 48.064801388861156\n",
      "Step: 1900 Current Loss: 0.9747806897734805 Average Loss: 49.273505376124405\n",
      "Step: 2000 Current Loss: 0.4564941293273786 Average Loss: 50.54011575505874\n",
      "Step: 2100 Current Loss: 3.169595086512089 Average Loss: 51.81399222148573\n",
      "Step: 2200 Current Loss: 0.6053439754338398 Average Loss: 53.02830280811128\n",
      "Step: 2300 Current Loss: 0.41193995600056793 Average Loss: 54.076295000283324\n",
      "Step: 2400 Current Loss: 1.1383231554270357 Average Loss: 55.415068754817696\n",
      "Step: 2500 Current Loss: 0.21361946932651119 Average Loss: 56.6112827904629\n",
      "Step: 2600 Current Loss: 0.6476945257262393 Average Loss: 57.8136517650062\n",
      "Step: 2700 Current Loss: 1.87978342507933 Average Loss: 58.87996081532179\n",
      "Step: 2800 Current Loss: 0.5597166575063061 Average Loss: 59.894036525403074\n",
      "Step: 2900 Current Loss: 1.9069606979625389 Average Loss: 60.879565856315125\n",
      "Step: 3000 Current Loss: 2.222236795895476 Average Loss: 61.97818177468056\n",
      "Step: 3100 Current Loss: 0.03353736133296055 Average Loss: 62.962031991318646\n",
      "Step: 3200 Current Loss: 1.6416318926436022 Average Loss: 63.86491023252042\n",
      "Step: 3300 Current Loss: 0.24816073800016608 Average Loss: 64.77971009680837\n",
      "Step: 3400 Current Loss: 0.8466459166592444 Average Loss: 65.59317575089055\n",
      "Step: 3500 Current Loss: 1.7463876166576575 Average Loss: 66.45008201444729\n",
      "Step: 3600 Current Loss: 0.21036453398358737 Average Loss: 67.39472131918816\n",
      "Step: 3700 Current Loss: 0.04591347830290777 Average Loss: 68.17121440142574\n",
      "Step: 3800 Current Loss: 0.13893511882671242 Average Loss: 68.93873749124226\n",
      "Step: 3900 Current Loss: 1.490543245240531 Average Loss: 69.84393024141843\n",
      "Step: 4000 Current Loss: 0.26225434230272765 Average Loss: 70.70239570523947\n",
      "Step: 4100 Current Loss: 0.39522311632979124 Average Loss: 71.67095487957472\n",
      "Step: 4200 Current Loss: 2.133453397040014 Average Loss: 72.63303241585857\n",
      "Step: 4300 Current Loss: 1.2564273494815683 Average Loss: 73.5496472310863\n",
      "Step: 4400 Current Loss: 1.7425378844831472 Average Loss: 74.3861034135255\n",
      "Step: 4500 Current Loss: 0.2602658061838461 Average Loss: 75.03359184645052\n",
      "Step: 4600 Current Loss: 0.20835765108179816 Average Loss: 75.87666398138121\n",
      "Step: 4700 Current Loss: 1.3565033646808087 Average Loss: 76.64187299981516\n",
      "Step: 4800 Current Loss: 0.8458310583195678 Average Loss: 77.46501157313827\n",
      "Step: 4900 Current Loss: 1.143934036878023 Average Loss: 78.18009838067445\n",
      "Step: 5000 Current Loss: 1.2503120983099019 Average Loss: 78.9719531104217\n",
      "Step: 5100 Current Loss: 2.0099362701819254 Average Loss: 79.82749179409883\n",
      "Step: 5200 Current Loss: 1.1351314198672253 Average Loss: 80.60358666653009\n",
      "Step: 5300 Current Loss: 1.1907731606502958 Average Loss: 81.53276166588027\n",
      "Step: 5400 Current Loss: 0.1118802714533471 Average Loss: 82.39582678782944\n",
      "Step: 5500 Current Loss: 1.485339837338123 Average Loss: 83.17197319905938\n",
      "Step: 5600 Current Loss: 0.43414069468841315 Average Loss: 83.91594496338897\n",
      "Step: 5700 Current Loss: 0.4833144424276143 Average Loss: 84.74404422051056\n",
      "Step: 5800 Current Loss: 0.17173703059580472 Average Loss: 85.35499050341859\n",
      "Step: 5900 Current Loss: 0.04560305324536339 Average Loss: 86.00656181832306\n",
      "Step: 6000 Current Loss: 1.0187062886548948 Average Loss: 86.67474041107793\n",
      "Step: 6100 Current Loss: 0.4780823431265392 Average Loss: 87.49724338768814\n",
      "Step: 6200 Current Loss: 1.2120261645465449 Average Loss: 88.27825386715023\n",
      "Step: 6300 Current Loss: 0.21334499610124594 Average Loss: 89.07457829626846\n",
      "Step: 6400 Current Loss: 0.8102227747659811 Average Loss: 89.75484113943182\n",
      "Step: 6500 Current Loss: 1.1981604828728805 Average Loss: 90.4922021768089\n",
      "Step: 6600 Current Loss: 1.5409532358733762 Average Loss: 91.22566424160408\n",
      "Step: 6700 Current Loss: 1.4721990926767679 Average Loss: 91.94295401876168\n",
      "Step: 6800 Current Loss: 0.6657826720784393 Average Loss: 92.5124459507165\n",
      "Step: 6900 Current Loss: 0.21219044256729697 Average Loss: 93.12572333569804\n",
      "Step: 7000 Current Loss: 0.548013945845695 Average Loss: 93.70822160291138\n",
      "Step: 7100 Current Loss: 1.4268768430210772 Average Loss: 94.46888442569049\n",
      "Step: 7200 Current Loss: 0.4004794876325384 Average Loss: 95.09389855449778\n",
      "Step: 7300 Current Loss: 0.8272516437997359 Average Loss: 95.66697154228662\n",
      "Step: 7400 Current Loss: 0.5769715251328061 Average Loss: 96.30731511826174\n",
      "Step: 7500 Current Loss: 0.7537492448071745 Average Loss: 96.94680915580966\n",
      "Step: 7600 Current Loss: 1.598527461272593 Average Loss: 97.55184890583126\n",
      "Step: 7700 Current Loss: 2.266210900288151 Average Loss: 98.13507012792147\n",
      "Step: 7800 Current Loss: 0.5349311771939981 Average Loss: 98.70071694151886\n",
      "Step: 7900 Current Loss: 0.9705836291880048 Average Loss: 99.28682401426575\n",
      "Step: 8000 Current Loss: 2.243420837095143 Average Loss: 99.98215673208493\n",
      "Step: 8100 Current Loss: 1.8202863102679494 Average Loss: 100.63914510814016\n",
      "Step: 8200 Current Loss: 0.16316006700580918 Average Loss: 101.15676253012366\n",
      "Step: 8300 Current Loss: 0.9596230854788571 Average Loss: 101.71423955196612\n",
      "Step: 8400 Current Loss: 0.3215765648449002 Average Loss: 102.2503830193571\n",
      "Step: 8500 Current Loss: 1.4425178621464687 Average Loss: 102.93430943162994\n",
      "Step: 8600 Current Loss: 0.10790088925630459 Average Loss: 103.46594725842415\n",
      "Step: 8700 Current Loss: 0.6855595730785307 Average Loss: 103.9557213786663\n",
      "Step: 8800 Current Loss: 0.6486443885204585 Average Loss: 104.43036670073155\n",
      "Step: 8900 Current Loss: 1.0081032753312984 Average Loss: 104.87503439163615\n",
      "Step: 9000 Current Loss: 0.26975807122830053 Average Loss: 105.31730472405631\n",
      "Step: 9100 Current Loss: 1.6176612183980084 Average Loss: 105.74787530307643\n",
      "Step: 9200 Current Loss: 0.8071091806043498 Average Loss: 106.20655458460868\n",
      "Step: 9300 Current Loss: 0.07641934846856688 Average Loss: 106.53974431668543\n",
      "Step: 9400 Current Loss: 0.1453946214298794 Average Loss: 106.96133587550304\n",
      "Step: 9500 Current Loss: 2.025100581445066 Average Loss: 107.51893322886696\n",
      "Step: 9600 Current Loss: 0.10833827795885244 Average Loss: 107.90111150112361\n",
      "Step: 9700 Current Loss: 1.5111185037701693 Average Loss: 108.21537873342811\n",
      "Step: 9800 Current Loss: 0.5498131227155689 Average Loss: 108.74094484684504\n",
      "Step: 9900 Current Loss: 0.22597908701933075 Average Loss: 109.09594781719515\n",
      "Step: 10000 Current Loss: 0.6565964019338871 Average Loss: 109.57537249447265\n",
      "Step: 10100 Current Loss: 0.8521651756221352 Average Loss: 109.96984683480704\n",
      "Step: 10200 Current Loss: 0.15915134789930746 Average Loss: 110.35560072719528\n",
      "Step: 10300 Current Loss: 1.020889432685211 Average Loss: 110.83665153081736\n",
      "Step: 10400 Current Loss: 0.9323034789629867 Average Loss: 111.2924471002178\n",
      "Step: 10500 Current Loss: 1.204983988473483 Average Loss: 111.75788174829292\n",
      "Step: 10600 Current Loss: 0.25980892298461206 Average Loss: 112.16640977917667\n",
      "Step: 10700 Current Loss: 0.7710767045410841 Average Loss: 112.49293707767593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 10800 Current Loss: 0.9880803914079919 Average Loss: 112.97902886430875\n",
      "Step: 10900 Current Loss: 1.111233048616819 Average Loss: 113.27462616471027\n",
      "Step: 11000 Current Loss: 0.1482761749886629 Average Loss: 113.76448935233628\n",
      "Step: 11100 Current Loss: 0.750734807146507 Average Loss: 114.20806252277455\n",
      "Step: 11200 Current Loss: 0.279166906049315 Average Loss: 114.60811404483137\n",
      "Step: 11300 Current Loss: 0.8423035840377141 Average Loss: 115.04630698652045\n",
      "Step: 11400 Current Loss: 0.9113743926795165 Average Loss: 115.51910294025545\n",
      "Step: 11500 Current Loss: 0.6492028542674129 Average Loss: 115.94548648202938\n",
      "Step: 11600 Current Loss: 1.0369517741981926 Average Loss: 116.31854751446812\n",
      "Step: 11700 Current Loss: 0.5740462574041378 Average Loss: 116.66774808944018\n",
      "Step: 11800 Current Loss: 0.1203534575418534 Average Loss: 116.98224812619607\n",
      "Step: 11900 Current Loss: 1.0765625637766938 Average Loss: 117.34018347340977\n",
      "Step: 12000 Current Loss: 0.3652678257223558 Average Loss: 117.71607773137659\n",
      "Step: 12100 Current Loss: 0.9591725443135899 Average Loss: 118.07338124399611\n",
      "Step: 12200 Current Loss: 0.20932479947420168 Average Loss: 118.47950104530851\n",
      "Step: 12300 Current Loss: 0.2168836154064438 Average Loss: 118.86570773008883\n",
      "Step: 12400 Current Loss: 0.26710050040618516 Average Loss: 119.36038140586675\n",
      "Step: 12500 Current Loss: 0.5556576779778892 Average Loss: 119.69406630489283\n",
      "Step: 12600 Current Loss: 2.513431095135421 Average Loss: 120.07134801397068\n",
      "Step: 12700 Current Loss: 0.3738351883342769 Average Loss: 120.4292271808782\n",
      "Step: 12800 Current Loss: 0.05490152237488747 Average Loss: 120.81049357513707\n",
      "Step: 12900 Current Loss: 0.124693236522522 Average Loss: 121.19452977183816\n",
      "Step: 13000 Current Loss: 1.8065670120268542 Average Loss: 121.4751054646051\n",
      "Step: 13100 Current Loss: 1.1795625084507115 Average Loss: 121.9556606465552\n",
      "Step: 13200 Current Loss: 0.1692842859111065 Average Loss: 122.2672985166649\n",
      "Step: 13300 Current Loss: 0.2581287630108447 Average Loss: 122.63381099605276\n",
      "Step: 13400 Current Loss: 0.0715011989209153 Average Loss: 123.02036787348361\n",
      "Step: 13500 Current Loss: 0.779178247510779 Average Loss: 123.30390272736118\n",
      "Step: 13600 Current Loss: 1.1176531704833161 Average Loss: 123.72352934718421\n",
      "Step: 13700 Current Loss: 0.8262304862217164 Average Loss: 124.05396466940651\n",
      "Step: 13800 Current Loss: 0.11464736958449287 Average Loss: 124.42045599662777\n",
      "Step: 13900 Current Loss: 1.2962439239159642 Average Loss: 124.78347745557342\n",
      "Step: 14000 Current Loss: 1.0727924182903326 Average Loss: 125.13986657883616\n",
      "Step: 14100 Current Loss: 1.6464782152550945 Average Loss: 125.45221141006625\n",
      "Step: 14200 Current Loss: 0.7907637150284965 Average Loss: 125.78001053937547\n",
      "Step: 14300 Current Loss: 1.225700687387841 Average Loss: 126.17417654155483\n",
      "Step: 14400 Current Loss: 0.44046725118916363 Average Loss: 126.56945297004269\n",
      "Step: 14500 Current Loss: 0.19451566128291276 Average Loss: 126.88622426988974\n",
      "Step: 14600 Current Loss: 0.8984317569417017 Average Loss: 127.2045783050075\n",
      "Step: 14700 Current Loss: 0.4789681439290079 Average Loss: 127.59394349098214\n",
      "Step: 14800 Current Loss: 0.6131464810752612 Average Loss: 127.9065778694617\n",
      "Step: 14900 Current Loss: 0.06536298955293203 Average Loss: 128.211273708832\n",
      "Step: 15000 Current Loss: 0.2517274127460987 Average Loss: 128.6055275642731\n",
      "Step: 15100 Current Loss: 1.5304275493083053 Average Loss: 128.9495678574829\n",
      "Step: 15200 Current Loss: 0.20247008690053284 Average Loss: 129.26803084051744\n",
      "Step: 15300 Current Loss: 1.2034493018052546 Average Loss: 129.58767330837907\n",
      "Step: 15400 Current Loss: 0.36945604635229295 Average Loss: 129.9082365571467\n",
      "Step: 15500 Current Loss: 0.10933900222462377 Average Loss: 130.27659731753008\n",
      "Step: 15600 Current Loss: 0.22086769137794188 Average Loss: 130.57274605626418\n",
      "Step: 15700 Current Loss: 1.9374125435694924 Average Loss: 130.87720914795196\n",
      "Step: 15800 Current Loss: 0.6047306153596924 Average Loss: 131.17972022586778\n",
      "Step: 15900 Current Loss: 0.9572150715677874 Average Loss: 131.55153646983416\n",
      "Step: 16000 Current Loss: 0.8265313589196642 Average Loss: 131.95761622718862\n",
      "Step: 16100 Current Loss: 0.15463582114412816 Average Loss: 132.3063850082513\n",
      "Step: 16200 Current Loss: 0.4008589572884926 Average Loss: 132.67129908617179\n",
      "Step: 16300 Current Loss: 0.29788063917016766 Average Loss: 133.10768552962648\n",
      "Step: 16400 Current Loss: 0.1286117769485783 Average Loss: 133.43521448383152\n",
      "Step: 16500 Current Loss: 0.8323164627185096 Average Loss: 133.82793276466322\n",
      "Step: 16600 Current Loss: 1.0466105638072403 Average Loss: 134.17166782042807\n",
      "Step: 16700 Current Loss: 1.9053866353035014 Average Loss: 134.52123579078372\n",
      "Step: 16800 Current Loss: 1.100975003940828 Average Loss: 134.83442811166267\n",
      "Step: 16900 Current Loss: 0.9386033853781484 Average Loss: 135.1498928778908\n",
      "Step: 17000 Current Loss: 0.40509216173942253 Average Loss: 135.4476076979885\n",
      "Step: 17100 Current Loss: 0.10048280435421754 Average Loss: 135.738843817153\n",
      "Step: 17200 Current Loss: 1.1716970449200697 Average Loss: 136.1207275076355\n",
      "Step: 17300 Current Loss: 0.6753495124569208 Average Loss: 136.51057235014008\n",
      "Step: 17400 Current Loss: 0.5486072331240192 Average Loss: 136.8313720207264\n",
      "Step: 17500 Current Loss: 1.594681402500275 Average Loss: 137.2116087713178\n",
      "Step: 17600 Current Loss: 0.2850533735741978 Average Loss: 137.51438853603338\n",
      "Step: 17700 Current Loss: 0.6758485697893567 Average Loss: 137.7851529623506\n",
      "Step: 17800 Current Loss: 1.0185105927040634 Average Loss: 138.18178802646483\n",
      "Step: 17900 Current Loss: 0.10085935203497493 Average Loss: 138.49683922186605\n",
      "Step: 18000 Current Loss: 1.003170697164381 Average Loss: 138.86015137074597\n",
      "Step: 18100 Current Loss: 0.9993702236035203 Average Loss: 139.20277070836954\n",
      "Step: 18200 Current Loss: 0.21187646159346893 Average Loss: 139.43596260281532\n",
      "Step: 18300 Current Loss: 0.13542939057479125 Average Loss: 139.78151290401794\n",
      "Step: 18400 Current Loss: 1.2283579076669315 Average Loss: 140.0877508366897\n",
      "Step: 18500 Current Loss: 0.6018122211486779 Average Loss: 140.44609922406542\n",
      "Step: 18600 Current Loss: 0.23709210198886882 Average Loss: 140.7445130370025\n",
      "Step: 18700 Current Loss: 0.07494281646182942 Average Loss: 141.0816313586737\n",
      "Step: 18800 Current Loss: 0.8056489564259539 Average Loss: 141.37322778047368\n",
      "Step: 18900 Current Loss: 0.9181427906469065 Average Loss: 141.67762511273887\n",
      "Step: 19000 Current Loss: 0.9094752908075265 Average Loss: 141.9591584336756\n",
      "Step: 19100 Current Loss: 1.6292811121280963 Average Loss: 142.3439823537646\n",
      "Step: 19200 Current Loss: 1.1355140032530835 Average Loss: 142.7020906491208\n",
      "Step: 19300 Current Loss: 0.13046531492927563 Average Loss: 143.08960746855516\n",
      "Step: 19400 Current Loss: 0.4713103477719617 Average Loss: 143.3809351353528\n",
      "Step: 19500 Current Loss: 0.8923973973562512 Average Loss: 143.68634253527543\n",
      "Step: 19600 Current Loss: 1.166065379855806 Average Loss: 143.99983145687887\n",
      "Step: 19700 Current Loss: 1.6049207521429587 Average Loss: 144.3442280693303\n",
      "Step: 19800 Current Loss: 1.2882678333389477 Average Loss: 144.71664620675003\n",
      "Step: 19900 Current Loss: 0.3337433534170601 Average Loss: 144.98685344994902\n",
      "Step: 20000 Current Loss: 0.9463249798739839 Average Loss: 145.34137163620665\n",
      "Step: 20100 Current Loss: 0.8096703333054331 Average Loss: 145.6676581644676\n",
      "Step: 20200 Current Loss: 1.3319433568035786 Average Loss: 145.96636775118202\n",
      "Step: 20300 Current Loss: 0.06897571631976157 Average Loss: 146.3116089678234\n",
      "Step: 20400 Current Loss: 1.0355116955441663 Average Loss: 146.61483232799182\n",
      "Step: 20500 Current Loss: 0.21473155592868767 Average Loss: 146.95406993651585\n",
      "Step: 20600 Current Loss: 0.9633569138868563 Average Loss: 147.2320291610863\n",
      "Step: 20700 Current Loss: 1.7107932511741926 Average Loss: 147.57639126365254\n",
      "Step: 20800 Current Loss: 0.39320383999606495 Average Loss: 147.9499771749347\n",
      "Step: 20900 Current Loss: 1.0050319612090592 Average Loss: 148.2780926410104\n",
      "Step: 21000 Current Loss: 1.3898271383037448 Average Loss: 148.5748060453543\n",
      "Step: 21100 Current Loss: 0.9444553756426693 Average Loss: 148.89690062401607\n",
      "Step: 21200 Current Loss: 1.0713173511131306 Average Loss: 149.22618641217815\n",
      "Step: 21300 Current Loss: 0.15346651892369176 Average Loss: 149.53418332884806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 21400 Current Loss: 0.28449742059687894 Average Loss: 149.827920127252\n",
      "Step: 21500 Current Loss: 2.2243872450719278 Average Loss: 150.17046697769018\n",
      "Step: 21600 Current Loss: 1.4620166182644443 Average Loss: 150.51951779777693\n",
      "Step: 21700 Current Loss: 0.8836848626083547 Average Loss: 150.91052195667837\n",
      "Step: 21800 Current Loss: 1.6236717974286488 Average Loss: 151.21003416274982\n",
      "Step: 21900 Current Loss: 1.0080660297830586 Average Loss: 151.55292422735505\n",
      "Step: 22000 Current Loss: 0.04427086372684202 Average Loss: 151.8633463297293\n",
      "Step: 22100 Current Loss: 0.17773831628604414 Average Loss: 152.1595618393546\n",
      "Step: 22200 Current Loss: 1.850757483143643 Average Loss: 152.5399578622242\n",
      "Step: 22300 Current Loss: 0.246051833987007 Average Loss: 152.7963064902626\n",
      "Step: 22400 Current Loss: 0.9880648584719707 Average Loss: 153.13361870446644\n",
      "Step: 22500 Current Loss: 1.5176834486117488 Average Loss: 153.4405937904418\n",
      "Step: 22600 Current Loss: 0.10637248149323003 Average Loss: 153.79764829912463\n",
      "Step: 22700 Current Loss: 1.2911202318158748 Average Loss: 154.12627115023983\n",
      "Step: 22800 Current Loss: 1.4534843530180388 Average Loss: 154.42281209764823\n",
      "Step: 22900 Current Loss: 0.1290500943300118 Average Loss: 154.73257121033254\n",
      "Step: 23000 Current Loss: 0.8271500909002101 Average Loss: 155.05512493823653\n",
      "Step: 23100 Current Loss: 1.5070927643698693 Average Loss: 155.34692515930632\n",
      "Step: 23200 Current Loss: 0.8099171041873844 Average Loss: 155.63246690936097\n",
      "Step: 23300 Current Loss: 1.6074438326195764 Average Loss: 155.953307502343\n",
      "Step: 23400 Current Loss: 1.1275695768005427 Average Loss: 156.28045279971514\n",
      "Step: 23500 Current Loss: 1.1337849473159678 Average Loss: 156.58895249781352\n",
      "Step: 23600 Current Loss: 0.803021943413974 Average Loss: 156.91755979934572\n",
      "Step: 23700 Current Loss: 0.10417986056843895 Average Loss: 157.23857551531577\n",
      "Step: 23800 Current Loss: 1.1664436354505017 Average Loss: 157.53817469894565\n",
      "Step: 23900 Current Loss: 1.5922929184034347 Average Loss: 157.81581431637582\n",
      "Step: 24000 Current Loss: 0.693989132376477 Average Loss: 158.10017033662783\n",
      "Step: 24100 Current Loss: 0.42071996681496227 Average Loss: 158.35985650771445\n",
      "Step: 24200 Current Loss: 0.35016011882241727 Average Loss: 158.63785362957955\n",
      "Step: 24300 Current Loss: 0.21185962716218046 Average Loss: 158.87198085558248\n",
      "Step: 24400 Current Loss: 2.39338512620852 Average Loss: 159.14819770837863\n",
      "Step: 24500 Current Loss: 0.23870052112646828 Average Loss: 159.4149167616326\n",
      "Step: 24600 Current Loss: 0.98392096558134 Average Loss: 159.7697342975731\n",
      "Step: 24700 Current Loss: 1.135912932476817 Average Loss: 160.02043705864344\n",
      "Step: 24800 Current Loss: 0.17487163457079608 Average Loss: 160.2833391943759\n",
      "Step: 24900 Current Loss: 1.0035919884283824 Average Loss: 160.61257785460927\n",
      "Step: 25000 Current Loss: 1.6203450010559362 Average Loss: 160.92880279287985\n",
      "Step: 25100 Current Loss: 0.7521143141579661 Average Loss: 161.20217794300288\n",
      "Step: 25200 Current Loss: 1.5957586685330651 Average Loss: 161.50989832634073\n",
      "Step: 25300 Current Loss: 0.09642543529322277 Average Loss: 161.8463640010125\n",
      "Step: 25400 Current Loss: 1.5011265202332302 Average Loss: 162.15793767990698\n",
      "Step: 25500 Current Loss: 0.7143414888802467 Average Loss: 162.46384435918068\n",
      "Step: 25600 Current Loss: 0.8222343098914596 Average Loss: 162.75571145095293\n",
      "Step: 25700 Current Loss: 1.9045534217969031 Average Loss: 163.04805593842588\n",
      "Step: 25800 Current Loss: 0.4378810854588247 Average Loss: 163.38779834773084\n",
      "Step: 25900 Current Loss: 1.079754231099891 Average Loss: 163.6487873877773\n",
      "Step: 26000 Current Loss: 0.3063084458871775 Average Loss: 163.9086655049179\n",
      "Step: 26100 Current Loss: 1.3918507644420215 Average Loss: 164.16423388006973\n",
      "Step: 26200 Current Loss: 0.9472217702007133 Average Loss: 164.49841211357133\n",
      "Step: 26300 Current Loss: 0.37453727745891124 Average Loss: 164.78315041795776\n",
      "Step: 26400 Current Loss: 0.8900292160642851 Average Loss: 165.06647099823633\n",
      "Step: 26500 Current Loss: 0.6072755939824851 Average Loss: 165.37560228725147\n",
      "Step: 26600 Current Loss: 0.6235198852139796 Average Loss: 165.69680968510545\n",
      "Step: 26700 Current Loss: 0.17970847646163204 Average Loss: 166.00469466820354\n",
      "Step: 26800 Current Loss: 0.16787983344921228 Average Loss: 166.25064542003153\n",
      "Step: 26900 Current Loss: 0.4533855346374466 Average Loss: 166.53592417600214\n",
      "Step: 27000 Current Loss: 1.0529340572442372 Average Loss: 166.8054459236144\n",
      "Step: 27100 Current Loss: 0.6126757724050268 Average Loss: 167.10428772057838\n",
      "Step: 27200 Current Loss: 0.9972865278291028 Average Loss: 167.34101781845948\n",
      "Step: 27300 Current Loss: 1.7460652931613165 Average Loss: 167.69143429411298\n",
      "Step: 27400 Current Loss: 0.6132668169272456 Average Loss: 167.99919013075018\n",
      "Step: 27500 Current Loss: 0.17749022637753928 Average Loss: 168.34516768274122\n",
      "Step: 27600 Current Loss: 1.4776291315315417 Average Loss: 168.68607853364873\n",
      "Step: 27700 Current Loss: 0.12669194341700274 Average Loss: 168.973635472312\n",
      "Step: 27800 Current Loss: 1.2547821956583585 Average Loss: 169.2779721170682\n",
      "Step: 27900 Current Loss: 0.25013296110580996 Average Loss: 169.63106868720553\n",
      "Step: 28000 Current Loss: 0.12031833121898423 Average Loss: 169.87776788512804\n",
      "Step: 28100 Current Loss: 1.2497096794252958 Average Loss: 170.16286725877592\n",
      "Step: 28200 Current Loss: 1.7848316299334408 Average Loss: 170.497134317322\n",
      "Step: 28300 Current Loss: 0.24532019780980205 Average Loss: 170.76957007786658\n",
      "Step: 28400 Current Loss: 1.4892660177353785 Average Loss: 171.28746841246826\n",
      "Step: 28500 Current Loss: 0.7268185554412753 Average Loss: 171.86288653699245\n",
      "Step: 28600 Current Loss: 2.3563296568277745 Average Loss: 172.36470199257883\n",
      "Step: 28700 Current Loss: 2.1161614370234774 Average Loss: 172.9683763128176\n",
      "Step: 28800 Current Loss: 0.8915133063182948 Average Loss: 173.4643478960532\n",
      "Step: 28900 Current Loss: 0.7648365387028327 Average Loss: 174.03619330480754\n",
      "Step: 29000 Current Loss: 2.318141448755233 Average Loss: 174.5193483854756\n",
      "Step: 29100 Current Loss: 0.6553287096064077 Average Loss: 175.08212819167076\n",
      "Step: 29200 Current Loss: 1.401300828053587 Average Loss: 175.67304898759366\n",
      "Step: 29300 Current Loss: 1.2616952709717155 Average Loss: 176.28202195946986\n",
      "Step: 29400 Current Loss: 0.4076525480737117 Average Loss: 176.7847403426568\n",
      "Step: 29500 Current Loss: 2.2230950376343794 Average Loss: 177.35427533127353\n",
      "Step: 29600 Current Loss: 0.2703389678877948 Average Loss: 177.92210196098907\n",
      "Step: 29700 Current Loss: 0.4279124255136729 Average Loss: 178.4262713253915\n",
      "Step: 29800 Current Loss: 0.24517853689247027 Average Loss: 179.00980866367397\n",
      "Step: 29900 Current Loss: 1.1828142251369138 Average Loss: 179.53985097797394\n",
      "Step: 30000 Current Loss: 1.072036860756628 Average Loss: 180.05396576794655\n",
      "Step: 30100 Current Loss: 1.5193378476181192 Average Loss: 180.54173142057334\n",
      "Step: 30200 Current Loss: 0.3680901398861273 Average Loss: 180.94387223407466\n",
      "Step: 30300 Current Loss: 0.5044130288324018 Average Loss: 181.49901312204352\n",
      "Step: 30400 Current Loss: 0.373813847424199 Average Loss: 182.01548678622387\n",
      "Step: 30500 Current Loss: 1.420599662320975 Average Loss: 182.56297462086505\n",
      "Step: 30600 Current Loss: 0.22722814215274756 Average Loss: 183.13094997380085\n",
      "Step: 30700 Current Loss: 0.5970816240524243 Average Loss: 183.52852992275575\n",
      "Step: 30800 Current Loss: 0.8479132114657071 Average Loss: 184.0829653805015\n",
      "Step: 30900 Current Loss: 0.24279439104146172 Average Loss: 184.55133444859538\n",
      "Step: 31000 Current Loss: 1.6470528645169136 Average Loss: 185.12260082895594\n",
      "Step: 31100 Current Loss: 0.4705434346057829 Average Loss: 185.66229708457215\n",
      "Step: 31200 Current Loss: 0.7934419003890326 Average Loss: 186.24142516238473\n",
      "Step: 31300 Current Loss: 1.1570600386687142 Average Loss: 186.77629354495633\n",
      "Step: 31400 Current Loss: 1.8943212029531347 Average Loss: 187.3582654325475\n",
      "Step: 31500 Current Loss: 0.2051491437193059 Average Loss: 187.79911448747424\n",
      "Step: 31600 Current Loss: 0.75867754838959 Average Loss: 188.27872921207936\n",
      "Step: 31700 Current Loss: 0.20356383763362337 Average Loss: 188.7708817864321\n",
      "Step: 31800 Current Loss: 2.1989523308587833 Average Loss: 189.31131607234818\n",
      "Step: 31900 Current Loss: 0.3548880247214041 Average Loss: 189.76976990665227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 32000 Current Loss: 0.8932662081127601 Average Loss: 190.21525337969138\n",
      "Step: 32100 Current Loss: 2.388120396359847 Average Loss: 190.7977796320859\n",
      "Step: 32200 Current Loss: 0.9781009077608318 Average Loss: 191.2628354012435\n",
      "Step: 32300 Current Loss: 1.1222689616416834 Average Loss: 191.79065919142354\n",
      "Step: 32400 Current Loss: 0.24421184140426197 Average Loss: 192.31669209390247\n",
      "Step: 32500 Current Loss: 0.17138082688389908 Average Loss: 192.72341303587385\n",
      "Step: 32600 Current Loss: 1.022877293966481 Average Loss: 193.23488768811052\n",
      "Step: 32700 Current Loss: 2.207233032315044 Average Loss: 193.74259624489747\n",
      "Step: 32800 Current Loss: 0.04284785551811867 Average Loss: 194.2704633371137\n",
      "Step: 32900 Current Loss: 0.19646803586310213 Average Loss: 194.76514896159262\n",
      "Step: 33000 Current Loss: 1.966588648442623 Average Loss: 195.27379662854304\n",
      "Step: 33100 Current Loss: 2.0550447687572926 Average Loss: 195.7476972879032\n",
      "Step: 33200 Current Loss: 0.14252725998970794 Average Loss: 196.33320032590464\n",
      "Step: 33300 Current Loss: 1.4143528852771752 Average Loss: 196.67590563100904\n",
      "Step: 33400 Current Loss: 0.5524451680045522 Average Loss: 196.91918879274343\n",
      "Step: 33500 Current Loss: 1.428603009850535 Average Loss: 197.18782444088632\n",
      "Step: 33600 Current Loss: 0.8497675980868452 Average Loss: 197.50530032196704\n",
      "Step: 33700 Current Loss: 0.7059652904742699 Average Loss: 197.75234158783437\n",
      "Step: 33800 Current Loss: 2.1561673045173904 Average Loss: 198.03588503560607\n",
      "Step: 33900 Current Loss: 0.4695207280618093 Average Loss: 198.32634717671343\n",
      "Step: 34000 Current Loss: 1.1748267233289813 Average Loss: 198.6336239203285\n",
      "Step: 34100 Current Loss: 0.4134800424828923 Average Loss: 198.88147614424045\n",
      "Step: 34200 Current Loss: 1.390942784652415 Average Loss: 199.14565694312296\n",
      "Step: 34300 Current Loss: 0.15183888780949759 Average Loss: 199.3982603268315\n",
      "Step: 34400 Current Loss: 0.15707905100646613 Average Loss: 199.66128919393097\n",
      "Step: 34500 Current Loss: 0.7768859652521866 Average Loss: 199.91165272316113\n",
      "Step: 34600 Current Loss: 0.7992196133325025 Average Loss: 200.14195521229865\n",
      "Step: 34700 Current Loss: 0.3702397155182006 Average Loss: 200.41304081601447\n",
      "Step: 34800 Current Loss: 0.5679161565811951 Average Loss: 200.7053168985037\n",
      "Step: 34900 Current Loss: 0.6539964483004004 Average Loss: 200.92094233662473\n",
      "Step: 35000 Current Loss: 0.7824635686206705 Average Loss: 201.14290915172802\n",
      "Step: 35100 Current Loss: 0.8178277327533038 Average Loss: 201.3375935318366\n",
      "Step: 35200 Current Loss: 1.5194951308640685 Average Loss: 201.6633827899734\n",
      "Step: 35300 Current Loss: 0.3002523941957865 Average Loss: 201.94191798183223\n",
      "Step: 35400 Current Loss: 1.2836560075291392 Average Loss: 202.18734778600873\n",
      "Step: 35500 Current Loss: 0.23431972309125046 Average Loss: 202.44695975006334\n",
      "Step: 35600 Current Loss: 0.4535302057380019 Average Loss: 202.7344778302505\n",
      "Step: 35700 Current Loss: 1.0530814247327644 Average Loss: 202.99644528961244\n",
      "Step: 35800 Current Loss: 0.7644539148619086 Average Loss: 203.26937973095494\n",
      "Step: 35900 Current Loss: 0.958663410903353 Average Loss: 203.5595691796764\n",
      "Step: 36000 Current Loss: 0.8510245476340652 Average Loss: 203.81603823065382\n",
      "Step: 36100 Current Loss: 1.5249123676380612 Average Loss: 204.07618593988573\n",
      "Step: 36200 Current Loss: 0.07162785766916849 Average Loss: 204.3345281296097\n",
      "Step: 36300 Current Loss: 0.8394671005694709 Average Loss: 204.56734730093416\n",
      "Step: 36400 Current Loss: 1.2686404168079615 Average Loss: 204.81279377521338\n",
      "Step: 36500 Current Loss: 0.6614726897560407 Average Loss: 205.09176741592282\n",
      "Step: 36600 Current Loss: 0.9954023605371753 Average Loss: 205.3564569095171\n",
      "Step: 36700 Current Loss: 0.15228950191370239 Average Loss: 205.63045604943676\n",
      "Step: 36800 Current Loss: 0.17957455703897715 Average Loss: 205.87590715125796\n",
      "Step: 36900 Current Loss: 1.0405418030706715 Average Loss: 206.13527505720995\n",
      "Step: 37000 Current Loss: 0.17576585300528075 Average Loss: 206.44265596485775\n",
      "Step: 37100 Current Loss: 1.216910556374783 Average Loss: 206.68327455078176\n",
      "Step: 37200 Current Loss: 1.2774562008854042 Average Loss: 207.01440131093148\n",
      "Step: 37300 Current Loss: 1.4279438913625555 Average Loss: 207.30571659736898\n",
      "Step: 37400 Current Loss: 0.9069979134966273 Average Loss: 207.5161149297787\n",
      "Step: 37500 Current Loss: 0.39487956016348774 Average Loss: 207.73148540970166\n",
      "Step: 37600 Current Loss: 0.12746675609853253 Average Loss: 208.01866385690818\n",
      "Step: 37700 Current Loss: 0.19488407558330603 Average Loss: 208.2343518256347\n",
      "Step: 37800 Current Loss: 1.4242178045698959 Average Loss: 208.548449939575\n",
      "Step: 37900 Current Loss: 0.07637433455821672 Average Loss: 208.79244888740848\n",
      "Step: 38000 Current Loss: 1.2427865072859552 Average Loss: 209.0468690320372\n",
      "Step: 38100 Current Loss: 0.09871968903741231 Average Loss: 209.3197398805141\n",
      "Step: 38200 Current Loss: 1.2561671470800322 Average Loss: 209.56481097359438\n",
      "Step: 38300 Current Loss: 0.2399783830234799 Average Loss: 209.79757005042458\n",
      "Step: 38400 Current Loss: 0.11603397318760592 Average Loss: 210.08976363084423\n",
      "Step: 38500 Current Loss: 1.4328183609305571 Average Loss: 210.46341569351569\n",
      "Step: 38600 Current Loss: 0.7389886413807665 Average Loss: 210.92850263093354\n",
      "Step: 38700 Current Loss: 0.3112624483017258 Average Loss: 211.3621667766561\n",
      "Step: 38800 Current Loss: 2.5869488981709234 Average Loss: 211.7803408087485\n",
      "Step: 38900 Current Loss: 0.9341410915551893 Average Loss: 212.2316953903318\n",
      "Step: 39000 Current Loss: 1.6317474616336027 Average Loss: 212.7110430235389\n",
      "Step: 39100 Current Loss: 0.19610188302342513 Average Loss: 213.06787164287215\n",
      "Step: 39200 Current Loss: 0.49525872902644597 Average Loss: 213.5187193502658\n",
      "Step: 39300 Current Loss: 0.05118948022502254 Average Loss: 213.8015955219364\n",
      "Step: 39400 Current Loss: 1.2758832211010165 Average Loss: 214.19658340654658\n",
      "Step: 39500 Current Loss: 2.6120120494050316 Average Loss: 214.66426012233777\n",
      "Step: 39600 Current Loss: 0.03336799259128271 Average Loss: 215.05542479345323\n",
      "Step: 39700 Current Loss: 0.5209088479286741 Average Loss: 215.37631594178887\n",
      "Step: 39800 Current Loss: 0.1806001078324435 Average Loss: 215.77276624857984\n",
      "Step: 39900 Current Loss: 0.8419303380046694 Average Loss: 216.1233871891022\n",
      "Step: 40000 Current Loss: 2.0915332845447114 Average Loss: 216.59309206355272\n",
      "Step: 40100 Current Loss: 0.3022148461102353 Average Loss: 216.94433398653166\n",
      "Step: 40200 Current Loss: 0.8745684482720287 Average Loss: 217.34382906755116\n",
      "Step: 40300 Current Loss: 1.7030665155254991 Average Loss: 217.73783782426992\n",
      "Step: 40400 Current Loss: 1.006376827749226 Average Loss: 218.1211589183282\n",
      "Step: 40500 Current Loss: 1.0951955241057074 Average Loss: 218.5378281060131\n",
      "Step: 40600 Current Loss: 1.3751054208958298 Average Loss: 218.94945023842953\n",
      "Step: 40700 Current Loss: 0.24887979814804023 Average Loss: 219.36158423572175\n",
      "loss: 219.6331001377207\n"
     ]
    }
   ],
   "source": [
    "#This code is inspired by the code from the Week 4 Discussion\n",
    "val_loader = DataLoader(val_dataset,batch_size=batch_size)\n",
    "\n",
    "val_loss = 0\n",
    "for i_batch, sample_batch in enumerate(val_loader):\n",
    "    inp, out = sample_batch\n",
    "    hidden_state = model.init_state()\n",
    "    inp = inp.reshape(50,2)\n",
    "    out = out.reshape(60,2)\n",
    "    for index in range(0,49):\n",
    "        output, hidden_state = model(inp[index].reshape(1,2).float(), hidden_state)\n",
    "    preds = torch.empty(0,2)\n",
    "    for outputIndex in range(0,60):\n",
    "        output, hidden_state = model(output, hidden_state)\n",
    "        preds = torch.cat((preds, output), 0)\n",
    "    loss = ((preds - out) ** 2).sum()\n",
    "    val_loss += loss.item()\n",
    "    if (i_batch % 100 == 0):\n",
    "        print('Step: {} Current Loss: {} Average Loss: {}'.format(i_batch, math.sqrt(loss.item() / len(val_dataset)), math.sqrt(val_loss / len(val_dataset))))\n",
    "print('loss: {}'.format(math.sqrt(val_loss / len(val_dataset))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86692b9a",
   "metadata": {},
   "source": [
    "## Test Algorithm and Convert to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "538afa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = np.zeros((test_dataset.shape[0], 120))\n",
    "for inputIndex in range(0, test_dataset[0].shape[0]):\n",
    "    hidden_state = model.init_state()\n",
    "    for index in range(0,49):\n",
    "        output, hidden_state = model(torch.tensor(test_dataset[inputIndex][index].reshape(1,2)).float(), hidden_state)\n",
    "    preds = torch.empty(0,2)\n",
    "    for outputIndex in range(0,60):\n",
    "        output, hidden_state = model(output, hidden_state)\n",
    "        preds = torch.cat((preds, output), 0)\n",
    "    output = preds.reshape(-1,120)\n",
    "    for outputIndex in range (0, 120):\n",
    "        outputs[inputIndex][outputIndex] = output[0][outputIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "236a8e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "columns = []\n",
    "for index in range(0, 120):\n",
    "    columns.append(\"v\" + str(index))\n",
    "citynames = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"]\n",
    "testDataAmounts = [6325, 7971, 6361, 3671, 3829, 1686]\n",
    "rows = []\n",
    "for arrayIndex in range(0, 6):\n",
    "    for itemIndex in range(0, testDataAmounts[arrayIndex]):\n",
    "        rows.append(str(itemIndex) + \"_\" + citynames[arrayIndex])\n",
    "\n",
    "df = pd.DataFrame(outputs, index=rows, columns=columns)\n",
    "df.to_csv('submission.csv', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38a5639",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
