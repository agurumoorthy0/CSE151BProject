{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "718c38cf",
   "metadata": {},
   "source": [
    "## Install the package dependencies before running this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16ac7530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    number of trajectories in each city\\n    # austin --  train: 43041 test: 6325 \\n    # miami -- train: 55029 test:7971\\n    # pittsburgh -- train: 43544 test: 6361\\n    # dearborn -- train: 24465 test: 3671\\n    # washington-dc -- train: 25744 test: 3829\\n    # palo-alto -- train:  11993 test:1686\\n\\n    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\\n    \\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "    number of trajectories in each city\n",
    "    # austin --  train: 43041 test: 6325 \n",
    "    # miami -- train: 55029 test:7971\n",
    "    # pittsburgh -- train: 43544 test: 6361\n",
    "    # dearborn -- train: 24465 test: 3671\n",
    "    # washington-dc -- train: 25744 test: 3829\n",
    "    # palo-alto -- train:  11993 test:1686\n",
    "\n",
    "    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b472cf2",
   "metadata": {},
   "source": [
    "## Create training, validation, and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "091abbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code is inspired by the code from the Week 7 Discussion\n",
    "from glob import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "ROOT_PATH = \"./\"\n",
    "\n",
    "cities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"]\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "def get_city_trajectories(city=\"palo-alto\", split=\"train\", normalized=False):\n",
    "    f_in = ROOT_PATH + \"train\" + \"/\" + city + \"_inputs\"\n",
    "    f_out = ROOT_PATH + \"train\" + \"/\" + city + \"_outputs\"\n",
    "    \n",
    "    inputs = None\n",
    "    outputs = None\n",
    "    \n",
    "    if city==\"all\":\n",
    "        allInputs = np.zeros((0,50,2))\n",
    "        allOutputs = np.zeros((0,60,2))\n",
    "        for city in cities:\n",
    "            if split==\"train\":\n",
    "                f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "                inputs = pickle.load(open(f_in, \"rb\"))\n",
    "                n = len(inputs)\n",
    "                allInputs = np.concatenate((allInputs, np.asarray(inputs)[:int(n * 0.8)]))\n",
    "\n",
    "                f_out = ROOT_PATH + split + \"/\" + city + \"_outputs\"\n",
    "                outputs = pickle.load(open(f_out, \"rb\"))\n",
    "                allOutputs = np.concatenate((allOutputs, np.asarray(outputs)[:int(n * 0.8)]))\n",
    "\n",
    "            elif split == 'val':\n",
    "                f_in = ROOT_PATH + 'train' + \"/\" + city + \"_inputs\"\n",
    "                inputs = pickle.load(open(f_in, \"rb\"))\n",
    "                n = len(inputs)\n",
    "                allInputs = np.concatenate((allInputs, np.asarray(inputs)[int(n * 0.8):]))\n",
    "\n",
    "                f_out = ROOT_PATH + 'train' + \"/\" + city + \"_outputs\"\n",
    "                outputs = pickle.load(open(f_out, \"rb\"))\n",
    "                allOutputs = np.concatenate((allOutputs, np.asarray(outputs)[int(n * 0.8):]))\n",
    "\n",
    "            else:\n",
    "                f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "                inputs = pickle.load(open(f_in, \"rb\"))\n",
    "                n = len(inputs)\n",
    "                allInputs = np.concatenate((allInputs, np.asarray(inputs)))\n",
    "                \n",
    "        if (normalized):\n",
    "            allInputs = (allInputs - np.min(allInputs))/(np.max(allInputs) - np.min(allInputs))\n",
    "            allOutputs = (allOutputs - np.min(allOutputs))/(np.max(allOutputs) - np.min(allOutputs))\n",
    "        return allInputs, allOutputs\n",
    "    \n",
    "    if split==\"train\":\n",
    "        inputs = pickle.load(open(f_in, \"rb\"))\n",
    "        n = len(inputs)\n",
    "        inputs = np.asarray(inputs)[:int(n * 0.8)]\n",
    "        \n",
    "        outputs = pickle.load(open(f_out, \"rb\"))\n",
    "        outputs = np.asarray(outputs)[:int(n * 0.8)]\n",
    "    \n",
    "    elif split==\"val\":\n",
    "        inputs = pickle.load(open(f_in, \"rb\"))\n",
    "        n = len(inputs)\n",
    "        inputs = np.asarray(inputs)[int(n * 0.8):]\n",
    "        \n",
    "        outputs = pickle.load(open(f_out, \"rb\"))\n",
    "        outputs = np.asarray(outputs)[int(n * 0.8):]\n",
    "    \n",
    "    else:\n",
    "        f_in = ROOT_PATH + spiit + \"/\" + city + \"_inputs\"\n",
    "        f_out = ROOT_PATH + split + \"/\" + city + \"_outputs\"\n",
    "        inputs = pickle.load(open(f_in, \"rb\"))\n",
    "        n = len(inputs)\n",
    "        inputs = np.asarray(inputs)\n",
    "        return inputs\n",
    "\n",
    "    return inputs, outputs\n",
    "\n",
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, city: str, split:str, transform=None):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.transform = transform\n",
    "\n",
    "        self.inputs, self.outputs = get_city_trajectories(city=city, split=split, normalized=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        data = (self.inputs[idx], self.outputs[idx])\n",
    "            \n",
    "        if self.transform:\n",
    "            data = torch.nn.functional.normalize(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "# intialize a dataset\n",
    "city = 'all' \n",
    "train_dataset  = ArgoverseDataset(city = city, split = \"train\")\n",
    "val_dataset = ArgoverseDataset(city = city, split = \"val\")\n",
    "test_dataset= get_city_trajectories(city = city, split = \"test\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058453cc",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5c14f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1  # batch size \n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "04f672e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is inspired by the code from cnvrg.io/pytorch-lstm/\n",
    "from torch import nn, optim\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, batch_size, output_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(98, output_size * 2)\n",
    "    \n",
    "    def forward(self, x, hidden_state, cell_state):        \n",
    "        output, (hidden, cell) = self.lstm(x, (hidden_state, cell_state))\n",
    "        output = output.reshape(-1, 49 * 2)\n",
    "        output = self.linear(output)\n",
    "        output = output.reshape(1, 60, 2)\n",
    "        return output, (hidden, cell)\n",
    "        \n",
    "    def init_state(self):\n",
    "        return torch.zeros(self.num_layers, self.batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ac3de7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.00001\n",
    "num_epochs = 1\n",
    "hidden_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5b77ca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "model = Model(2, hidden_size, 1, batch_size, 60)\n",
    "opt = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819d015a",
   "metadata": {},
   "source": [
    "## Train and Validate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2cd4c8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0  RMSE: 1.6560723869561664 MSE: 2.7425757508386948\n",
      "step 1000  RMSE: 114.30771874308073 MSE: 13066.254564247249\n",
      "step 2000  RMSE: 159.03592262521926 MSE: 25292.42468525473\n",
      "step 3000  RMSE: 194.92416205273832 MSE: 37995.42895196219\n",
      "step 4000  RMSE: 224.97028771907188 MSE: 50611.630356401976\n",
      "step 5000  RMSE: 249.05391772433842 MSE: 62027.853933841536\n",
      "step 6000  RMSE: 270.2085339105224 MSE: 73012.65179807394\n",
      "step 7000  RMSE: 288.8171574813556 MSE: 83415.35045561015\n",
      "step 8000  RMSE: 306.12078967041305 MSE: 93709.93786843725\n",
      "step 9000  RMSE: 321.0985534795301 MSE: 103104.28104664663\n",
      "step 10000  RMSE: 335.6247183509674 MSE: 112643.95156816617\n",
      "step 11000  RMSE: 350.037717264371 MSE: 122526.40350765173\n",
      "step 12000  RMSE: 362.7867458066366 MSE: 131614.22293296916\n",
      "step 13000  RMSE: 374.64197861917864 MSE: 140356.6121436931\n",
      "step 14000  RMSE: 384.469773310998 MSE: 147817.0065898102\n",
      "step 15000  RMSE: 394.73922753182404 MSE: 155819.05775242115\n",
      "step 16000  RMSE: 403.50209706801166 MSE: 162813.94233828312\n",
      "step 17000  RMSE: 412.7504592453675 MSE: 170362.94160726178\n",
      "step 18000  RMSE: 421.1741235471133 MSE: 177387.64234567908\n",
      "step 19000  RMSE: 429.38666608627733 MSE: 184372.90901268824\n",
      "step 20000  RMSE: 437.0610279404076 MSE: 191022.34214432575\n",
      "step 21000  RMSE: 444.419045889052 MSE: 197508.28834893528\n",
      "step 22000  RMSE: 451.9169907478906 MSE: 204228.96652662905\n",
      "step 23000  RMSE: 458.59460617453163 MSE: 210309.01281237375\n",
      "step 24000  RMSE: 464.9360250418571 MSE: 216165.50738172236\n",
      "step 25000  RMSE: 470.95908574997236 MSE: 221802.46045044984\n",
      "step 26000  RMSE: 477.87093699213693 MSE: 228360.63242174292\n",
      "step 27000  RMSE: 483.7871359903428 MSE: 234049.99294973843\n",
      "step 28000  RMSE: 489.5872632159248 MSE: 239695.6883032592\n",
      "step 29000  RMSE: 495.09965380941526 MSE: 245123.66720220284\n",
      "step 30000  RMSE: 500.55007780707666 MSE: 250550.38039267051\n",
      "step 31000  RMSE: 505.90000889819703 MSE: 255934.81900319585\n",
      "step 32000  RMSE: 511.0581444253344 MSE: 261180.4269834659\n",
      "step 33000  RMSE: 516.0623993111888 MSE: 266320.39998282085\n",
      "step 34000  RMSE: 520.872248919318 MSE: 271307.899694268\n",
      "step 35000  RMSE: 558.9546239952608 MSE: 312430.2716856834\n",
      "step 36000  RMSE: 615.301158921904 MSE: 378595.51617063815\n",
      "step 37000  RMSE: 668.3615763885517 MSE: 446707.1967925898\n",
      "step 38000  RMSE: 717.4465841974173 MSE: 514729.6011765418\n",
      "step 39000  RMSE: 763.0284737100769 MSE: 582212.4516923296\n",
      "step 40000  RMSE: 806.490569812131 MSE: 650427.0391958957\n",
      "step 41000  RMSE: 847.825658494542 MSE: 718808.3472017038\n",
      "step 42000  RMSE: 886.7351951892291 MSE: 786299.3063872802\n",
      "step 43000  RMSE: 923.5788111271619 MSE: 852997.8203630617\n",
      "step 44000  RMSE: 959.8171273534294 MSE: 921248.9179609894\n",
      "step 45000  RMSE: 993.3226960779214 MSE: 986689.9785435104\n",
      "step 46000  RMSE: 1025.121620602918 MSE: 1050874.3370275532\n",
      "step 47000  RMSE: 1054.4882096985561 MSE: 1111945.384393266\n",
      "step 48000  RMSE: 1085.3827546035554 MSE: 1178055.7239908017\n",
      "step 49000  RMSE: 1115.0539349797266 MSE: 1243345.2779137725\n",
      "step 50000  RMSE: 1143.6630978937724 MSE: 1307965.2814839806\n",
      "step 51000  RMSE: 1170.4937494382884 MSE: 1370055.6174741026\n",
      "step 52000  RMSE: 1194.6327487421356 MSE: 1427147.4043671903\n",
      "step 53000  RMSE: 1220.041151089693 MSE: 1488500.410352263\n",
      "step 54000  RMSE: 1245.589165404883 MSE: 1551492.3689740333\n",
      "step 55000  RMSE: 1269.468710125156 MSE: 1611550.805986827\n",
      "step 56000  RMSE: 1292.8759697792862 MSE: 1671528.2732327294\n",
      "step 57000  RMSE: 1315.9022652635435 MSE: 1731598.7717257254\n",
      "step 58000  RMSE: 1337.8367491258962 MSE: 1789807.1673117462\n",
      "step 59000  RMSE: 1359.8638953162556 MSE: 1849229.8137847\n",
      "step 60000  RMSE: 1381.7854369643696 MSE: 1909330.993806814\n",
      "step 61000  RMSE: 1401.7026955341264 MSE: 1964770.446667636\n",
      "step 62000  RMSE: 1420.8871007648484 MSE: 2018920.1531199366\n",
      "step 63000  RMSE: 1441.2487376524778 MSE: 2077197.9237848604\n",
      "step 64000  RMSE: 1460.2439486635844 MSE: 2132312.389608617\n",
      "step 65000  RMSE: 1479.533356210331 MSE: 2189018.9521390065\n",
      "step 66000  RMSE: 1498.0941957877237 MSE: 2244286.2194528664\n",
      "step 67000  RMSE: 1516.5078353321508 MSE: 2299796.014623806\n",
      "step 68000  RMSE: 1533.7698846644962 MSE: 2352450.0591037422\n",
      "step 69000  RMSE: 1551.906873358968 MSE: 2408414.9435788076\n",
      "step 70000  RMSE: 1569.3785794757653 MSE: 2462949.125717371\n",
      "step 71000  RMSE: 1586.1735395118951 MSE: 2515946.4974476933\n",
      "step 72000  RMSE: 1603.2501620690675 MSE: 2570411.0821744916\n",
      "step 73000  RMSE: 1619.7981607056654 MSE: 2623746.0814254563\n",
      "step 74000  RMSE: 1635.1999916855161 MSE: 2673879.012808312\n",
      "step 75000  RMSE: 1651.4029183908863 MSE: 2727131.598869936\n",
      "step 76000  RMSE: 1666.5350774758979 MSE: 2777339.1644575973\n",
      "step 77000  RMSE: 1682.2423126823778 MSE: 2829939.198578955\n",
      "step 78000  RMSE: 1697.9116674468355 MSE: 2882904.030452093\n",
      "step 79000  RMSE: 1709.7118175210817 MSE: 2923114.4989712406\n",
      "step 80000  RMSE: 1718.5667191469295 MSE: 2953471.5681594415\n",
      "step 81000  RMSE: 1726.5595566922477 MSE: 2981007.9028053307\n",
      "step 82000  RMSE: 1735.2282237327684 MSE: 3011016.988438778\n",
      "step 83000  RMSE: 1743.5330709925815 MSE: 3039907.5696448227\n",
      "step 84000  RMSE: 1751.2824396098429 MSE: 3066990.1832858026\n",
      "step 85000  RMSE: 1759.257223334004 MSE: 3094985.97785287\n",
      "step 86000  RMSE: 1767.0087910792968 MSE: 3122320.0677515175\n",
      "step 87000  RMSE: 1774.3682451422426 MSE: 3148382.6693691616\n",
      "step 88000  RMSE: 1781.3249490330368 MSE: 3173118.574047551\n",
      "step 89000  RMSE: 1788.5334221888231 MSE: 3198851.802286463\n",
      "step 90000  RMSE: 1795.1377729747453 MSE: 3222519.6239607283\n",
      "step 91000  RMSE: 1802.1172892991149 MSE: 3247626.72439079\n",
      "step 92000  RMSE: 1809.0739830154703 MSE: 3272748.676023458\n",
      "step 93000  RMSE: 1815.388679749679 MSE: 3295636.058563283\n",
      "step 94000  RMSE: 1821.375330227243 MSE: 3317408.0935603986\n",
      "step 95000  RMSE: 1827.6612125701877 MSE: 3340345.507933529\n",
      "step 96000  RMSE: 1833.5360867472496 MSE: 3361854.5814044178\n",
      "step 97000  RMSE: 1839.1740920034788 MSE: 3382561.3406968205\n",
      "step 98000  RMSE: 1845.1525728951424 MSE: 3404588.0172615633\n",
      "step 99000  RMSE: 1850.8839866679136 MSE: 3425771.53210371\n",
      "step 100000  RMSE: 1856.4674532918475 MSE: 3446471.4051319184\n",
      "step 101000  RMSE: 1861.774462333348 MSE: 3466204.1485966276\n",
      "step 102000  RMSE: 1867.212872158401 MSE: 3486483.9099540254\n",
      "step 103000  RMSE: 1872.7190756644425 MSE: 3507076.736357484\n",
      "step 104000  RMSE: 1878.003972506058 MSE: 3526898.920748535\n",
      "step 105000  RMSE: 1882.9703585602444 MSE: 3545577.3712164955\n",
      "step 106000  RMSE: 1887.957462340862 MSE: 3564383.3796085473\n",
      "step 107000  RMSE: 1892.7540267323166 MSE: 3582517.805711399\n",
      "step 108000  RMSE: 1897.1053586636122 MSE: 3599008.7418701923\n",
      "step 109000  RMSE: 1901.8066282617488 MSE: 3616868.4513003216\n",
      "step 110000  RMSE: 1906.4862514285014 MSE: 3634689.826885899\n",
      "step 111000  RMSE: 1911.348738016275 MSE: 3653253.998316407\n",
      "step 112000  RMSE: 1916.1114610672362 MSE: 3671483.1312332186\n",
      "step 113000  RMSE: 1920.578615427136 MSE: 3688622.2180360146\n",
      "step 114000  RMSE: 1950.9313227558257 MSE: 3806133.0261097956\n",
      "step 115000  RMSE: 1988.9887984461984 MSE: 3956076.4403444524\n",
      "step 116000  RMSE: 2025.9567064415542 MSE: 4104500.57637551\n",
      "step 117000  RMSE: 2062.681544428124 MSE: 4254655.153724392\n",
      "step 118000  RMSE: 2097.1327162935504 MSE: 4397965.629748764\n",
      "step 119000  RMSE: 2132.8549987865713 MSE: 4549070.4458488645\n",
      "step 120000  RMSE: 2166.207962084427 MSE: 4692456.934997966\n",
      "step 121000  RMSE: 2197.822764191049 MSE: 4830424.902796384\n",
      "step 122000  RMSE: 2229.9247428653534 MSE: 4972564.358843112\n",
      "step 123000  RMSE: 2260.0849354085053 MSE: 5107983.915260468\n",
      "step 124000  RMSE: 2290.675688110986 MSE: 5247195.108102738\n",
      "step 125000  RMSE: 2320.784450600914 MSE: 5386040.466150987\n",
      "step 126000  RMSE: 2350.123522504473 MSE: 5523080.571028831\n",
      "step 127000  RMSE: 2379.3202861548357 MSE: 5661165.024107929\n",
      "step 128000  RMSE: 2407.3965943334974 MSE: 5795558.3624085225\n",
      "step 129000  RMSE: 2435.8271704174917 MSE: 5933254.004144084\n",
      "step 130000  RMSE: 2462.606934751281 MSE: 6064432.915085101\n",
      "step 131000  RMSE: 2488.727943768122 MSE: 6193766.778092304\n",
      "step 132000  RMSE: 2515.015249808505 MSE: 6325301.706769336\n",
      "step 133000  RMSE: 2536.6634262724606 MSE: 6434661.33818834\n",
      "step 134000  RMSE: 2539.7948797747345 MSE: 6450558.031329959\n",
      "step 135000  RMSE: 2542.9820120284526 MSE: 6466757.513500278\n",
      "step 136000  RMSE: 2546.2232269506335 MSE: 6483252.721462897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 137000  RMSE: 2549.2577386998087 MSE: 6498715.018320863\n",
      "step 138000  RMSE: 2552.193308731317 MSE: 6513690.685132906\n",
      "step 139000  RMSE: 2555.2583549303454 MSE: 6529345.260441334\n",
      "step 140000  RMSE: 2558.2663945237928 MSE: 6544726.945349765\n",
      "step 141000  RMSE: 2560.958935039558 MSE: 6558510.666958947\n",
      "step 142000  RMSE: 2563.7951258048433 MSE: 6573045.447100672\n",
      "step 143000  RMSE: 2566.492068929612 MSE: 6586881.5398786\n",
      "step 144000  RMSE: 2569.325568174251 MSE: 6601433.875273939\n",
      "step 145000  RMSE: 2572.114259495393 MSE: 6615771.763899535\n",
      "step 146000  RMSE: 2574.9353295719075 MSE: 6630291.951477587\n",
      "step 147000  RMSE: 2577.6793088783975 MSE: 6644430.619419813\n",
      "step 148000  RMSE: 2580.253890848548 MSE: 6657710.141239071\n",
      "step 149000  RMSE: 2582.893298789657 MSE: 6671337.792932517\n",
      "step 150000  RMSE: 2585.5156784878363 MSE: 6684891.323706416\n",
      "step 151000  RMSE: 2587.993742717382 MSE: 6697711.612344323\n",
      "step 152000  RMSE: 2590.6038437004195 MSE: 6711228.274995388\n",
      "step 153000  RMSE: 2593.007349902173 MSE: 6723687.11664669\n",
      "step 154000  RMSE: 2595.776161928671 MSE: 6738053.882837144\n",
      "step 155000  RMSE: 2598.464511743075 MSE: 6752017.818788176\n",
      "step 156000  RMSE: 2601.02504554111 MSE: 6765331.287532133\n",
      "step 157000  RMSE: 2603.373669792071 MSE: 6777554.464566634\n",
      "step 158000  RMSE: 2605.7190292811347 MSE: 6789771.659557819\n",
      "step 159000  RMSE: 2607.926098494129 MSE: 6801278.535206809\n",
      "step 160000  RMSE: 2610.035673440508 MSE: 6812286.216632047\n",
      "step 161000  RMSE: 2612.1374859817583 MSE: 6823262.2456711\n",
      "step 162000  RMSE: 2614.0510396134828 MSE: 6833262.837704331\n",
      "step 163000  RMSE: 2616.042476588798 MSE: 6843678.239316852\n",
      "805.5053398609161\n",
      "epoch 0  RMSE: 2616.142897450935 MSE: 6844203.659882973\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    if (epoch == 0):\n",
    "        startTime = time.time()\n",
    "    \n",
    "    total_loss = 0\n",
    "    for i_batch, sample_batch in enumerate(train_loader):\n",
    "        hidden_state = model.init_state()\n",
    "        cell_state = model.init_state()\n",
    "        inp, out = sample_batch\n",
    "        if (inp.shape[0] == batch_size):\n",
    "            modelInput = inp[:,:49,:]\n",
    "            preds, (hidden_state, cell_state) = model(modelInput.reshape(batch_size, 49, 2).float(), hidden_state, cell_state)\n",
    "            loss = criterion(preds, out.float())        \n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item()\n",
    "            if (i_batch % 1000 == 0):\n",
    "                print('step {}  RMSE: {} MSE: {}'.format(i_batch, math.sqrt(total_loss / len(train_dataset)), total_loss / len(train_dataset)))\n",
    "    if (epoch == 0):\n",
    "        endTime = time.time()\n",
    "        print(endTime - startTime)\n",
    "    print('epoch {}  RMSE: {} MSE: {}'.format(epoch, math.sqrt(total_loss / len(train_dataset)), total_loss / len(train_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "34bd8a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0  RMSE: 3.7489020743625714 MSE: 14.054266763159992\n",
      "step 1000  RMSE: 113.99485911318169 MSE: 12994.827904234144\n",
      "step 2000  RMSE: 159.76461369025654 MSE: 25524.731787596906\n",
      "step 3000  RMSE: 192.74318064206795 MSE: 37149.933684020834\n",
      "step 4000  RMSE: 222.6709489748429 MSE: 49582.35151735709\n",
      "step 5000  RMSE: 250.32932792005337 MSE: 62664.77241690561\n",
      "step 6000  RMSE: 274.80729065893763 MSE: 75519.04699930582\n",
      "step 7000  RMSE: 298.252908849201 MSE: 88954.7976370098\n",
      "step 8000  RMSE: 317.0495113154559 MSE: 100520.39262536941\n",
      "step 9000  RMSE: 362.5456397183412 MSE: 131439.34087878125\n",
      "step 10000  RMSE: 438.57939102939434 MSE: 192351.88223571438\n",
      "step 11000  RMSE: 501.06247441340145 MSE: 251063.60326528057\n",
      "step 12000  RMSE: 527.6317290949979 MSE: 278395.24154777726\n",
      "step 13000  RMSE: 554.6987175807301 MSE: 307690.6672857066\n",
      "step 14000  RMSE: 579.659447031494 MSE: 336005.07453285734\n",
      "step 15000  RMSE: 604.4671011067082 MSE: 365380.47632034746\n",
      "step 16000  RMSE: 626.2867395905854 MSE: 392235.08018700575\n",
      "step 17000  RMSE: 647.1726554400701 MSE: 418832.44594935165\n",
      "step 18000  RMSE: 668.0635780039597 MSE: 446308.94425545284\n",
      "step 19000  RMSE: 687.6488743130903 MSE: 472860.9743440603\n",
      "step 20000  RMSE: 702.8571003651924 MSE: 494008.10353376623\n",
      "step 21000  RMSE: 711.647259316598 MSE: 506441.8216928253\n",
      "step 22000  RMSE: 720.0480703576288 MSE: 518469.22362574475\n",
      "step 23000  RMSE: 728.4154865951559 MSE: 530589.1211116577\n",
      "step 24000  RMSE: 736.7648126697695 MSE: 542822.3891883206\n",
      "step 25000  RMSE: 745.2129835276401 MSE: 555342.3908181668\n",
      "step 26000  RMSE: 755.5837541988635 MSE: 570906.8096092487\n",
      "step 27000  RMSE: 764.0219910151899 MSE: 583729.6027548149\n",
      "step 28000  RMSE: 772.77628737523 MSE: 597183.1903294441\n",
      "step 29000  RMSE: 822.437961317808 MSE: 676404.2002165922\n",
      "step 30000  RMSE: 884.6304456826482 MSE: 782571.0254286807\n",
      "step 31000  RMSE: 944.1873652483416 MSE: 891489.7806946052\n",
      "step 32000  RMSE: 997.4958112720631 MSE: 994997.8935053112\n",
      "step 33000  RMSE: 1046.8870213977846 MSE: 1095972.4355711252\n",
      "step 34000  RMSE: 1061.4409612084182 MSE: 1126656.914131051\n",
      "step 35000  RMSE: 1066.9396474139198 MSE: 1138360.2112237394\n",
      "step 36000  RMSE: 1071.7014989760505 MSE: 1148544.1029075135\n",
      "step 37000  RMSE: 1076.6405273960493 MSE: 1159154.825231643\n",
      "step 38000  RMSE: 1081.7547632575709 MSE: 1170193.3678304432\n",
      "step 39000  RMSE: 1086.6945669210436 MSE: 1180905.0817757146\n",
      "step 40000  RMSE: 1091.1665083832315 MSE: 1190644.3490172527\n",
      "loss: 2188.8460916007675\n"
     ]
    }
   ],
   "source": [
    "val_loader = DataLoader(val_dataset,batch_size=batch_size)\n",
    "\n",
    "val_loss = 0\n",
    "for i_batch, sample_batch in enumerate(val_loader):\n",
    "    hidden_state = model.init_state()\n",
    "    cell_state = model.init_state()\n",
    "    inp, out = sample_batch\n",
    "    if (inp.shape[0] == batch_size):\n",
    "        modelInput = inp[:,:49,:]\n",
    "        preds, (hidden_state, cell_state) = model(modelInput.reshape(batch_size, 49, 2).float(), hidden_state, cell_state)\n",
    "        loss = criterion(preds, out.float())        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        val_loss += loss.item()\n",
    "        if (i_batch % 1000 == 0):\n",
    "            print('step {}  RMSE: {} MSE: {}'.format(i_batch, math.sqrt(val_loss / len(train_dataset)), val_loss / len(train_dataset)))\n",
    "\n",
    "print('loss: {}'.format(math.sqrt(val_loss / len(val_dataset))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86692b9a",
   "metadata": {},
   "source": [
    "## Test Algorithm and Convert to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "538afa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = np.zeros((test_dataset.shape[0], 120))\n",
    "for inputIndex in range(0, test_dataset.shape[0]):\n",
    "    hidden_state = model.init_state()\n",
    "    cell_state = model.init_state()\n",
    "    output, (hidden_state, cell_state) = model(torch.tensor(test_dataset[inputIndex][:49]).reshape(1, 49, 2).float(), hidden_state, cell_state)\n",
    "    output = output.reshape(1,120)\n",
    "    for outputIndex in range (0, 120):\n",
    "        outputs[inputIndex][outputIndex] = output[0][outputIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "236a8e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "columns = []\n",
    "for index in range(0, 120):\n",
    "    columns.append(\"v\" + str(index))\n",
    "citynames = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"]\n",
    "testDataAmounts = [6325, 7971, 6361, 3671, 3829, 1686]\n",
    "rows = []\n",
    "for arrayIndex in range(0, 6):\n",
    "    for itemIndex in range(0, testDataAmounts[arrayIndex]):\n",
    "        rows.append(str(itemIndex) + \"_\" + citynames[arrayIndex])\n",
    "\n",
    "df = pd.DataFrame(outputs, index=rows, columns=columns)\n",
    "df.to_csv('submission.csv', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38a5639",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
