{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "718c38cf",
   "metadata": {},
   "source": [
    "## Install the package dependencies before running this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ac7530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    number of trajectories in each city\\n    # austin --  train: 43041 test: 6325 \\n    # miami -- train: 55029 test:7971\\n    # pittsburgh -- train: 43544 test: 6361\\n    # dearborn -- train: 24465 test: 3671\\n    # washington-dc -- train: 25744 test: 3829\\n    # palo-alto -- train:  11993 test:1686\\n\\n    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\\n    \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "    number of trajectories in each city\n",
    "    # austin --  train: 43041 test: 6325 \n",
    "    # miami -- train: 55029 test:7971\n",
    "    # pittsburgh -- train: 43544 test: 6361\n",
    "    # dearborn -- train: 24465 test: 3671\n",
    "    # washington-dc -- train: 25744 test: 3829\n",
    "    # palo-alto -- train:  11993 test:1686\n",
    "\n",
    "    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b472cf2",
   "metadata": {},
   "source": [
    "## Create training, validation, and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "091abbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code is inspired by the code from the Week 7 Discussion\n",
    "from glob import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "ROOT_PATH = \"./\"\n",
    "\n",
    "cities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"]\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "def get_city_trajectories(city=\"palo-alto\", split=\"train\", normalized=False):\n",
    "    f_in = ROOT_PATH + \"train\" + \"/\" + city + \"_inputs\"\n",
    "    f_out = ROOT_PATH + \"train\" + \"/\" + city + \"_outputs\"\n",
    "    \n",
    "    inputs = None\n",
    "    outputs = None\n",
    "    \n",
    "    if city==\"all\":\n",
    "        allInputs = np.zeros((0,50,2))\n",
    "        allOutputs = np.zeros((0,60,2))\n",
    "        for city in cities:\n",
    "            if split==\"train\":\n",
    "                f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "                inputs = pickle.load(open(f_in, \"rb\"))\n",
    "                n = len(inputs)\n",
    "                allInputs = np.concatenate((allInputs, np.asarray(inputs)[:int(n * 0.8)]))\n",
    "\n",
    "                f_out = ROOT_PATH + split + \"/\" + city + \"_outputs\"\n",
    "                outputs = pickle.load(open(f_out, \"rb\"))\n",
    "                allOutputs = np.concatenate((allOutputs, np.asarray(outputs)[:int(n * 0.8)]))\n",
    "\n",
    "            elif split == 'val':\n",
    "                f_in = ROOT_PATH + 'train' + \"/\" + city + \"_inputs\"\n",
    "                inputs = pickle.load(open(f_in, \"rb\"))\n",
    "                n = len(inputs)\n",
    "                allInputs = np.concatenate((allInputs, np.asarray(inputs)[int(n * 0.8):]))\n",
    "\n",
    "                f_out = ROOT_PATH + 'train' + \"/\" + city + \"_outputs\"\n",
    "                outputs = pickle.load(open(f_out, \"rb\"))\n",
    "                allOutputs = np.concatenate((allOutputs, np.asarray(outputs)[int(n * 0.8):]))\n",
    "\n",
    "            else:\n",
    "                f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "                inputs = pickle.load(open(f_in, \"rb\"))\n",
    "                n = len(inputs)\n",
    "                allInputs = np.concatenate((allInputs, np.asarray(inputs)))\n",
    "                \n",
    "\n",
    "        return allInputs, allOutputs\n",
    "    \n",
    "    if split==\"train\":\n",
    "        inputs = pickle.load(open(f_in, \"rb\"))\n",
    "        n = len(inputs)\n",
    "        inputs = np.asarray(inputs)[:int(n * 0.8)]\n",
    "        \n",
    "        outputs = pickle.load(open(f_out, \"rb\"))\n",
    "        outputs = np.asarray(outputs)[:int(n * 0.8)]\n",
    "    \n",
    "    elif split==\"val\":\n",
    "        inputs = pickle.load(open(f_in, \"rb\"))\n",
    "        n = len(inputs)\n",
    "        inputs = np.asarray(inputs)[int(n * 0.8):]\n",
    "        \n",
    "        outputs = pickle.load(open(f_out, \"rb\"))\n",
    "        outputs = np.asarray(outputs)[int(n * 0.8):]\n",
    "    \n",
    "    else:\n",
    "        f_in = ROOT_PATH + spiit + \"/\" + city + \"_inputs\"\n",
    "        f_out = ROOT_PATH + split + \"/\" + city + \"_outputs\"\n",
    "        inputs = pickle.load(open(f_in, \"rb\"))\n",
    "        n = len(inputs)\n",
    "        inputs = np.asarray(inputs)\n",
    "        return inputs\n",
    "\n",
    "    return inputs, outputs\n",
    "\n",
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, city: str, split:str, transform=None):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        \n",
    "        self.inputs, self.outputs = get_city_trajectories(city=city, split=split, normalized=False)\n",
    "        self.inputMin = np.min(self.inputs)\n",
    "        self.inputMax = np.max(self.inputs)\n",
    "        self.outputMin = np.min(self.outputs)\n",
    "        self.outputMax = np.max(self.outputs)\n",
    "        \n",
    "        if (transform == \"normalization\"):\n",
    "            self.inputs = (self.inputs - self.inputMin)/(self.inputMax - self.inputMin)\n",
    "            self.outputs = (self.outputs - self.outputMin)/(self.outputMax - self.outputMin)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        data = (self.inputs[idx], self.outputs[idx])\n",
    "        return data\n",
    "    \n",
    "    def __getmin__(self):\n",
    "        return self.inputMin, self.outputMin\n",
    "    \n",
    "    def __getmax__(self):\n",
    "        return self.inputMax, self.outputMax\n",
    "\n",
    "# intialize a dataset\n",
    "city = 'all' \n",
    "train_dataset = ArgoverseDataset(city = city, split = \"train\", transform=\"normalization\")\n",
    "val_dataset = ArgoverseDataset(city = city, split = \"val\")\n",
    "test_dataset = get_city_trajectories(city = city, split = \"test\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058453cc",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c14f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 20  # batch size \n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04f672e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is inspired by the code from the Week 7 Discussion\n",
    "from torch import nn, optim\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(100, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 120)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 100).float()\n",
    "        x = self.model(x)\n",
    "        x = x.reshape(-1, 60, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac3de7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.00001\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b77ca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "opt = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819d015a",
   "metadata": {},
   "source": [
    "## Train and Validate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cd4c8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.594635009765625\n",
      "epoch 0  RMSE: 0.20788813833643913 MSE: 0.04321747806099045\n",
      "epoch 1  RMSE: 0.05746834620873278 MSE: 0.003302610815966771\n",
      "epoch 2  RMSE: 0.046632813601574036 MSE: 0.0021746193043991483\n",
      "epoch 3  RMSE: 0.04583665353528963 MSE: 0.0021009988073141792\n",
      "epoch 4  RMSE: 0.032241872123890576 MSE: 0.0010395383180533122\n",
      "epoch 5  RMSE: 0.02949035712487335 MSE: 0.0008696811633525684\n",
      "epoch 6  RMSE: 0.028347452434539292 MSE: 0.0008035780595284675\n",
      "epoch 7  RMSE: 0.029364722459496163 MSE: 0.0008622869251232384\n",
      "epoch 8  RMSE: 0.02575943838880572 MSE: 0.0006635486661066779\n",
      "epoch 9  RMSE: 0.028162760560564645 MSE: 0.0007931410823916955\n",
      "epoch 10  RMSE: 0.02660919001067763 MSE: 0.0007080489930243462\n",
      "epoch 11  RMSE: 0.024397691346997992 MSE: 0.0005952473430633807\n",
      "epoch 12  RMSE: 0.02363853247793355 MSE: 0.0005587802177103192\n",
      "epoch 13  RMSE: 0.023151104094703424 MSE: 0.0005359736208037937\n",
      "epoch 14  RMSE: 0.022915272559595985 MSE: 0.0005251097164805727\n",
      "epoch 15  RMSE: 0.022798335881598138 MSE: 0.0005197641189701651\n",
      "epoch 16  RMSE: 0.022320443989697368 MSE: 0.0004982022198972174\n",
      "epoch 17  RMSE: 0.02190134535938699 MSE: 0.00047966892855114205\n",
      "epoch 18  RMSE: 0.021865531552224836 MSE: 0.0004781014700613398\n",
      "epoch 19  RMSE: 0.02154088961740583 MSE: 0.00046400992550926215\n",
      "epoch 20  RMSE: 0.021168680857558554 MSE: 0.0004481130492491659\n",
      "epoch 21  RMSE: 0.020923333565389342 MSE: 0.00043778588748854825\n",
      "epoch 22  RMSE: 0.020876867771225126 MSE: 0.0004358436079372184\n",
      "epoch 23  RMSE: 0.020526586047459498 MSE: 0.000421340734763759\n",
      "epoch 24  RMSE: 0.02036523902212922 MSE: 0.0004147429604284547\n",
      "epoch 25  RMSE: 0.020275538566208534 MSE: 0.0004110974641498096\n",
      "epoch 26  RMSE: 0.01996135530429088 MSE: 0.00039845570558414166\n",
      "epoch 27  RMSE: 0.01973958443683604 MSE: 0.00038965119373897964\n",
      "epoch 28  RMSE: 0.01947518106476559 MSE: 0.00037928267750540415\n",
      "epoch 29  RMSE: 0.019260287853327264 MSE: 0.00037095868819302574\n",
      "epoch 30  RMSE: 0.019208576637853123 MSE: 0.0003689694164522767\n",
      "epoch 31  RMSE: 0.019048229546793177 MSE: 0.00036283504886732457\n",
      "epoch 32  RMSE: 0.018982347179968582 MSE: 0.0003603295044608612\n",
      "epoch 33  RMSE: 0.018596622200864454 MSE: 0.0003458343572816847\n",
      "epoch 34  RMSE: 0.018433650767270965 MSE: 0.0003397994806097095\n",
      "epoch 35  RMSE: 0.018288434901382468 MSE: 0.0003344668511421044\n",
      "epoch 36  RMSE: 0.018292316721096304 MSE: 0.00033460885102489944\n",
      "epoch 37  RMSE: 0.017900492961486214 MSE: 0.00032042764826421745\n",
      "epoch 38  RMSE: 0.017724090671130233 MSE: 0.0003141433901184457\n",
      "epoch 39  RMSE: 0.017573825524677962 MSE: 0.0003088393435718227\n",
      "epoch 40  RMSE: 0.01731820079372886 MSE: 0.0002999200787319109\n",
      "epoch 41  RMSE: 0.01715678367845772 MSE: 0.00029435522618939317\n",
      "epoch 42  RMSE: 0.01698198773117195 MSE: 0.0002883879073016746\n",
      "epoch 43  RMSE: 0.016807725283385765 MSE: 0.00028249962920176515\n",
      "epoch 44  RMSE: 0.016641200058823503 MSE: 0.0002769295393977873\n",
      "epoch 45  RMSE: 0.01647353752995629 MSE: 0.0002713774387508784\n",
      "epoch 46  RMSE: 0.016356286579887556 MSE: 0.0002675281106834097\n",
      "epoch 47  RMSE: 0.016239052746360737 MSE: 0.00026370683409908624\n",
      "epoch 48  RMSE: 0.016225988157844668 MSE: 0.00026328269169851546\n",
      "epoch 49  RMSE: 0.0160289237435701 MSE: 0.0002569263963771853\n"
     ]
    }
   ],
   "source": [
    "#This code is inspired by the code from the Week 7 Discussion\n",
    "import time\n",
    "import math\n",
    "\n",
    "indexArray = []\n",
    "lossArray = []\n",
    "for epoch in range(num_epochs):\n",
    "    if (epoch == 0):\n",
    "        startTime = time.time()\n",
    "    \n",
    "    total_loss = 0\n",
    "    for i_batch, sample_batch in enumerate(train_loader):\n",
    "        inp, out = sample_batch\n",
    "        preds = model(inp)\n",
    "        loss = ((preds - out) ** 2).sum()\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    if (epoch == 0):\n",
    "        endTime = time.time()\n",
    "        print(endTime - startTime)\n",
    "    \n",
    "    indexArray.append(epoch)\n",
    "    lossArray.append( math.sqrt(total_loss / len(train_dataset )) )\n",
    "    \n",
    "    print('epoch {}  RMSE: {} MSE: {}'.format(epoch, math.sqrt(total_loss / len(train_dataset)), total_loss / len(train_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34bd8a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 11062.964825373198\n"
     ]
    }
   ],
   "source": [
    "#This code is inspired by the code from the Week 7 Discussion\n",
    "val_loader = DataLoader(val_dataset,batch_size=batch_sz)\n",
    "\n",
    "val_loss = 0\n",
    "for i_batch, sample_batch in enumerate(val_loader):\n",
    "    inp, out = sample_batch\n",
    "    preds = model(inp)\n",
    "    loss = ((preds - out) ** 2).sum()\n",
    "    val_loss += loss.item()\n",
    "print('loss: {}'.format(math.sqrt(val_loss / len(val_dataset))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86692b9a",
   "metadata": {},
   "source": [
    "## Test Algorithm and Convert to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "538afa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = np.zeros((test_dataset.shape[0], 120))\n",
    "for inputIndex in range(0, test_dataset.shape[0]):\n",
    "        output = model(torch.tensor(test_dataset[inputIndex]))\n",
    "        output = output.reshape(-1,120)\n",
    "        for outputIndex in range (0, 120):\n",
    "            outputs[inputIndex][outputIndex] = output[0][outputIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "236a8e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "columns = []\n",
    "for index in range(0, 120):\n",
    "    columns.append(\"v\" + str(index))\n",
    "citynames = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"]\n",
    "testDataAmounts = [6325, 7971, 6361, 3671, 3829, 1686]\n",
    "rows = []\n",
    "for arrayIndex in range(0, 6):\n",
    "    for itemIndex in range(0, testDataAmounts[arrayIndex]):\n",
    "        rows.append(str(itemIndex) + \"_\" + citynames[arrayIndex])\n",
    "\n",
    "df = pd.DataFrame(outputs, index=rows, columns=columns)\n",
    "df.to_csv('submission.csv', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36fb7f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
